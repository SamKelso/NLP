{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H08esTFOYO99"
   },
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z_h7jlePHNXF",
    "outputId": "095a5f94-ebe7-48ff-ec08-bfcb59d0cf31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simpletransformers\n",
      "  Downloading simpletransformers-0.63.9-py3-none-any.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.5/250.5 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting streamlit\n",
      "  Downloading streamlit-1.19.0-py2.py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.28.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.1.2)\n",
      "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.21.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.4.0)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.12.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.1.97)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.9.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.64.1)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.13.4)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.23.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.9.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (23.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb>=0.10.32->simpletransformers) (1.14.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (66.1.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.19.6)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.0.11)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.30)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->simpletransformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->simpletransformers) (2.8)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (3.8.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.70.13)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (10.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting validators>=0.2\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (5.3.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (6.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (13.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (9.2.0)\n",
      "Collecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tzlocal>=1.1\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Collecting semver\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting blinker>=1.0.0\n",
      "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-2.3.1-py3-none-manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.51.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.16.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.17.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.14.0)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.11.0->streamlit->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.2)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.7-py2.py3-none-any.whl (340 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.1/340.1 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval, validators\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=40bc9a38d31e23b2fff396d86ac928a86b2648fad03b9c6d42f5c9ea0e1b83e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9c/d6/00/1ccfd5a7466a94774e00022683d4b028836032dfb85007822b\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19580 sha256=797245a2ae3a8ba8cd08cfbea068bdd952707d3e54bd0dbabdb98a0bbe9a457c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/6a/fd/17a2b6f8d376e336d46414eeaf656327fd973158159b059046\n",
      "Successfully built seqeval validators\n",
      "Installing collected packages: watchdog, validators, tzdata, toml, semver, pympler, blinker, pytz-deprecation-shim, pydeck, tzlocal, seqeval, altair, streamlit, simpletransformers\n",
      "Successfully installed altair-4.2.2 blinker-1.5 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 semver-2.13.0 seqeval-1.2.2 simpletransformers-0.63.9 streamlit-1.19.0 toml-0.10.2 tzdata-2022.7 tzlocal-4.2 validators-0.20.0 watchdog-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorboardx\n",
      "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (3.19.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (1.23.4)\n",
      "Installing collected packages: tensorboardx\n",
      "Successfully installed tensorboardx-2.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install tensorboardx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hYhFR7nSYOjG",
    "outputId": "8cab9fb6-a017-4c4c-abdc-cac74ccd32c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.9/dist-packages (2.6)\n",
      "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (3.19.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardx) (23.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (8.0.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (5.8.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (6.16.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets) (8.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.4)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.6.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mEnabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardx\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install ipywidgets\n",
    "!pip install nltk\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMQDATlOZHxu"
   },
   "source": [
    "# Fetch Don't Patronize Me! data manager module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4PRFUZQ5RwA",
    "outputId": "19b9c4e9-d494-414c-afcf-b94b5017f61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW903YxwThrH",
    "outputId": "23d3f520-4530-4271-821b-ecacdd420d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from collections import Counter\n",
    "from urllib import request\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/dont_patronize_me.py\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))\n",
    "\n",
    "\n",
    "# helper function to save predictions to an output file\n",
    "def labels2file(p, outf_path):\n",
    "\twith open(outf_path,'w') as outf:\n",
    "\t\tfor pi in p:\n",
    "\t\t\toutf.write(','.join([str(k) for k in pi])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0YcdU80IbiS"
   },
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Xp3MMJx6cazQ",
    "outputId": "3bccfd99-6fb1-400d-e063-f0288c569eed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>community</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>1</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>1</td>\n",
       "      <td>refugee</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>1</td>\n",
       "      <td>refugee</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>1</td>\n",
       "      <td>in-need</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>1</td>\n",
       "      <td>homeless</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   par_id                                               text  label community  \\\n",
       "0    4046  We also know that they can benefit by receivin...      1  hopeless   \n",
       "1    1279  Pope Francis washed and kissed the feet of Mus...      1   refugee   \n",
       "2    8330  Many refugees do n't want to be resettled anyw...      1   refugee   \n",
       "3    4063  \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...      1   in-need   \n",
       "4    4089  \"In a 90-degree view of his constituency , one...      1  homeless   \n",
       "\n",
       "   length  \n",
       "0     493  \n",
       "1     197  \n",
       "2      74  \n",
       "3     218  \n",
       "4     355  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the training and development data set from cv file to save time\n",
    "# Thiis is the original dataset\n",
    "# train_df=pd.read_csv('/content/drive/MyDrive/NLPClassification_40/train_df_.csv')\n",
    "# test_df=pd.read_csv('/content/drive/MyDrive/NLPClassification_40/test_df_.csv')\n",
    "\n",
    "# # This is the upsampled augmented dataset\n",
    "train_df=pd.read_csv('back_translation_german_french.csv')\n",
    "val_df = pd.read_csv('val_set1.csv')\n",
    "test_df=pd.read_csv('test_df_.csv')\n",
    "\n",
    "\n",
    "submission_test_df = pd.read_csv('task4_test.tsv', sep='\\t', names=['par_id', 'art_id', 'keyword', 'country', 'text']).drop('art_id', axis=1)\n",
    "submission_test_df['length'] = submission_test_df.apply(lambda x: len(x['text']), axis=1)\n",
    "submission_test_df.rename(columns={\"keyword\": \"community\"}, inplace=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "CPQKmncUialF",
    "outputId": "8f78eca7-1101-4e41-f5ff-4816f5fdd97b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>community</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>687</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>Changing the rule would n't prevent H-1B spous...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5592</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>According to Caribbean Policy Research Institu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7251</td>\n",
       "      <td>poor-families</td>\n",
       "      <td>Schott says Illinois has made it easier for fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2986</td>\n",
       "      <td>women</td>\n",
       "      <td>It will also promote freedom for women to part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9345</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>God is God Foundation ( GIG ) , a non-governme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>805</td>\n",
       "      <td>women</td>\n",
       "      <td>Even when it 's about the ideas , when If Ther...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>531</td>\n",
       "      <td>in-need</td>\n",
       "      <td>According to the Disaster Management Centre ( ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>792</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>Answer : Though the wording is slightly differ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2869</td>\n",
       "      <td>refugee</td>\n",
       "      <td>\"\"\" Despite the rising number of refugees acro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>6592</td>\n",
       "      <td>women</td>\n",
       "      <td>North and South Korea have agreed in principle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1257 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      community  \\\n",
       "0        687      immigrant   \n",
       "1       5592  poor-families   \n",
       "2       7251  poor-families   \n",
       "3       2986          women   \n",
       "4       9345     vulnerable   \n",
       "...      ...            ...   \n",
       "1252     805          women   \n",
       "1253     531        in-need   \n",
       "1254     792      immigrant   \n",
       "1255    2869        refugee   \n",
       "1256    6592          women   \n",
       "\n",
       "                                                   text  label  \n",
       "0     Changing the rule would n't prevent H-1B spous...      0  \n",
       "1     According to Caribbean Policy Research Institu...      0  \n",
       "2     Schott says Illinois has made it easier for fa...      0  \n",
       "3     It will also promote freedom for women to part...      0  \n",
       "4     God is God Foundation ( GIG ) , a non-governme...      1  \n",
       "...                                                 ...    ...  \n",
       "1252  Even when it 's about the ideas , when If Ther...      0  \n",
       "1253  According to the Disaster Management Centre ( ...      0  \n",
       "1254  Answer : Though the wording is slightly differ...      0  \n",
       "1255  \"\"\" Despite the rising number of refugees acro...      0  \n",
       "1256  North and South Korea have agreed in principle...      0  \n",
       "\n",
       "[1257 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQDgEBan1RDh",
    "outputId": "28ac9253-d29b-497e-ee6f-427cf0000d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953\n"
     ]
    }
   ],
   "source": [
    "# shuffling the training dataset\n",
    "train_dataframe = train_df.sample(frac=1).reset_index(drop=True)\n",
    "val_dataframe = val_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Peprocess the data\n",
    "train_dataframe = pd.get_dummies(train_dataframe, columns=['community']).reset_index(drop=True)\n",
    "val_set = pd.get_dummies(val_dataframe, columns=['community']).reset_index(drop=True)\n",
    "test_set = pd.get_dummies(test_df, columns=['community']).reset_index(drop=True)\n",
    "submission_set = pd.get_dummies(submission_test_df, columns=['community']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# split\n",
    "# downsample negative instances\n",
    "pcldf = train_dataframe[train_dataframe.label==1]\n",
    "npos = len(pcldf)\n",
    "print(npos)\n",
    "ratio = 4\n",
    "# training_set = train_dataframe#pd.concat([pcldf,train_dataframe[train_dataframe.label==0][:npos*class_ratio]],ignore_index=True)\n",
    "training_set = pd.concat([pcldf,pcldf,pcldf, train_dataframe[train_dataframe.label==0]], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# #creating the validation set\n",
    "# val_pcl_size = int(npos * 0.15)\n",
    "# val_pcldf = pcldf[-val_pcl_size:].reset_index(drop=True)\n",
    "# val_set = pd.concat([val_pcldf, val_dataframe[val_dataframe.label==0][npos * 4: npos * 4 + val_pcl_size * 3]], ignore_index=True)\n",
    "\n",
    "\n",
    "# Normalize_length\n",
    "# max_length = 2160\n",
    "# adding length does not improve results\n",
    "# training_set['length'] = training_set.apply(lambda x: x['length'] / max_length, axis=1).reset_index(drop=True)\n",
    "# val_set['length'] = val_set.apply(lambda x: x['length'] / max_length, axis=1).reset_index(drop=True)\n",
    "# test_set['length'] = test_set.apply(lambda x: x['length'] / max_length, axis=1).reset_index(drop=True)\n",
    "# submission_test_df['length'] = submission_test_df.apply(lambda x: x['length'] / max_length, axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#additional inputs to the model\n",
    "# extra_inputs = ['length']\n",
    "community_features=[]\n",
    "community_features.extend([column for column in training_set.columns if column.startswith('community_')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J8gXHaV3Qma",
    "outputId": "661a802b-aebf-4aa3-f8dd-ba0e8c919a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 12321. PCL class count : 5859. Non PCL class count= 6462\n",
      "Validation set size: 1257. PCL class count: 138\n",
      "Test set size: 2093. PCL class count: 199\n",
      "Submission test set size: 3832\n"
     ]
    }
   ],
   "source": [
    "train_size, train_pcl,train_non_pcl = len(training_set), len(training_set[training_set.label==1]),len(training_set[training_set.label==0])\n",
    "val_size, val_pcl = len(val_set), len(val_set[val_set.label==1])\n",
    "test_size, test_pcl = len(test_set), len(test_set[test_set.label==1])\n",
    "sub_size = len(submission_test_df)\n",
    "\n",
    "print(f\"Training set size: {train_size}. PCL class count : {train_pcl}. Non PCL class count= {train_non_pcl}\")\n",
    "print(f\"Validation set size: {val_size}. PCL class count: {val_pcl}\")\n",
    "print(f\"Test set size: {test_size}. PCL class count: {test_pcl}\")\n",
    "print(f\"Submission test set size: {sub_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj9zcW_hiMbd"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6I1yHiw_nr4j",
    "outputId": "a9cda12a-db1d-496a-e8fc-8fad2b043390"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "import random\n",
    "import os\n",
    "from urllib import request\n",
    "\n",
    "#Imports for Transformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs, MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "\n",
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, BertPreTrainedModel\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaTokenizerFast, RobertaPreTrainedModel, RobertaForSequenceClassification\n",
    "\n",
    "!mkdir ref res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KnNdtFej2Q2Z"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, input_set, include_community=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = input_set['text']\n",
    "        self.labels = input_set['label']\n",
    "\n",
    "        self.include_community = include_community\n",
    "        \n",
    "        if self.include_community:\n",
    "            self.community_labels = [torch.tensor(e) for e in input_set[community_features].values]\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for sample in batch:\n",
    "            texts.append(sample['input_ids'])\n",
    "            labels.append(sample['label'])\n",
    "\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        encodings['label'] =  torch.tensor(labels)\n",
    "\n",
    "        if self.include_community:\n",
    "            community_label = []\n",
    "            for sample in batch:\n",
    "                community_label.append(sample['community_label'])\n",
    "            encodings['community_label'] = torch.vstack(community_label)\n",
    "        \n",
    "        return encodings\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError\n",
    "        item = {\n",
    "            'input_ids': self.texts[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "        if self.include_community:\n",
    "            item['community_label'] = self.community_labels[idx]\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VGwuwRK2nsK8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class RobertaCustom(RobertaPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        self.config = config\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "\n",
    "            torch.nn.Linear(config.hidden_size + len(community_features), config.hidden_size + len(community_features)),\n",
    "            nn.Tanh(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "\n",
    "            torch.nn.Linear(config.hidden_size + len(community_features), 2)\n",
    "        )\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, community_label, input_ids=None, **kwargs):\n",
    "        outputs = self.roberta(input_ids, **kwargs)\n",
    "\n",
    "        embeddings = outputs[0][:, 0, :]\n",
    "        concat = torch.cat((embeddings, community_label), dim=-1)\n",
    "\n",
    "        return self.classifier(concat.float())\n",
    "\n",
    "\n",
    "class TrainerCustom(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # weight_dict = {0:1.0, 1:self.pos_weight}\n",
    "        # weights = compute_class_weight(weight_dict, classes=np.unique(training_set.label), y=training_set.label)\n",
    "        # loss = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).cuda())\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        label = inputs.pop('label')\n",
    "        logits = model(**inputs, return_dict=False)\n",
    "        \n",
    "        predictions = torch.clone(logits[0]) if isinstance(logits, tuple) else torch.clone(logits)\n",
    "        outputs = {'predictions':predictions, 'labels':label}\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, 2), label.view(-1))\n",
    "        \n",
    "        \n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=8, shuffle=True, collate_fn=self.train_dataset.collate_fn)\n",
    "\n",
    "\n",
    "class CustomLossCallback(TrainerCallback):\n",
    "    def __init__(self, val_dataset, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.f1_scores = []\n",
    "\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "#     def on_epoch_end(self, args, state, control, **kwargs):\n",
    "#         # For final model training set includes validation, do not pay attention to scores\n",
    "#         model = kwargs['model']\n",
    "#         loss = nn.CrossEntropyLoss()\n",
    "\n",
    "#         self.train_losses.append(state.log_history[-1]['loss'])\n",
    "#         val_losses = []\n",
    "#         preds = []\n",
    "#         labels = []\n",
    "#         print(state.log_history[-1])\n",
    "#         with torch.no_grad():\n",
    "#             for batch in self.val_loader:\n",
    "#                 batch = batch.to(torch.device('cuda'))\n",
    "#                 label = batch.pop('label')\n",
    "#                 community_label = batch.pop('community_label')\n",
    "                \n",
    "#                 logits = model(community_label, **batch)\n",
    "                \n",
    "#                 val_losses.append(loss(logits.view(-1, 2), label.view(-1)).cpu().numpy())\n",
    "                \n",
    "#                 preds.extend(list(logits.argmax(dim=-1).cpu().numpy()))\n",
    "#                 labels.extend(list(label.cpu().numpy()))\n",
    "\n",
    "#         self.val_losses.append(np.mean(val_losses))\n",
    "#         self.f1_scores.append(f1_score(labels, preds, zero_division=0))\n",
    "\n",
    "#         print('Train loss: {}, Val loss: {}, Val F1: {}'.format(self.train_losses[-1], self.val_losses[-1], self.f1_scores[-1]))\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % 201 == 0:\n",
    "            self.train_losses.append(state.log_history[-2]['loss'])\n",
    "        \n",
    "    \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        #print(state.log_history[-1])\n",
    "        #self.train_losses.append(state.log_history[-1]['loss'])\n",
    "        self.val_losses.append(metrics['eval_loss'])\n",
    "        self.f1_scores.append(metrics['eval_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NLuvfvg1dt7N"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "def compute_metrics(p:transformers.EvalPrediction):\n",
    "    logits, labels = p.predictions\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    logits = torch.Tensor(logits).view(-1,2)\n",
    "    labels = torch.Tensor(labels).view(-1).long()\n",
    "    val_loss = loss(logits, labels).cpu()\n",
    "\n",
    "    #preds = torch.Tensor(logits)\n",
    "    preds = torch.nn.Sigmoid()(logits)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    recall = recall_score(y_true=labels, y_pred=preds)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)    \n",
    "    return {\"loss\": val_loss, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0nivKOaiUan"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QHRnqC26sjZO"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "iUeONa64c_TK"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-W_c4Ocz6E7"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "#config.num_hidden_layers=18\n",
    "\n",
    "model = RobertaPCL.from_pretrained('roberta-base',config=config)\n",
    "\n",
    "train_dataset = PCLDataset(tokenizer, training_set, augment=True)\n",
    "val_dataset = PCLDataset(tokenizer, val_set, augment=True)\n",
    "\n",
    "learning_rate_curve =LearningCurveCallback(val_dataset)\n",
    "\n",
    "num_epochs = 2\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/content/drive/MyDrive/NLPClassification_40/experiment/PCL',\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=num_epochs\n",
    ")\n",
    "\n",
    "trainer = TrainerPCL(\n",
    "    model=model,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,                   \n",
    "    data_collator=train_dataset.collate_fn,\n",
    "    optimizers=(AdamW(model.parameters(), lr=3e-5), None),\n",
    "    callbacks=[learning_rate_curve]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('/content/drive/MyDrive/NLPClassification_40/models/pcl_roberta_finetuned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdr6O0s7l2MN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzJXWhXmceXj"
   },
   "source": [
    "### **OPTUNA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOrNxovUr-P4",
    "outputId": "43cd4034-43ab-4331-cb39-4fc97a4acc6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.23.4)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.41)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.64.1)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.5/210.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.9.4 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267,
     "referenced_widgets": [
      "3a6f951883ce43bebbb76640b0eaa8c5",
      "0f21c31d526546c3b5e6c679a8c40060",
      "39d2f76ced4d434ba462d61554d85bff",
      "48d9b89e8d814695bd607f73cb5a37ad",
      "9e99cea34c0c4cb9a34f3ef66b922c6b",
      "22346120d609431eb7933db08c41092c",
      "a9a8248b51da49dca255ca56f751453e",
      "3918797b878946fb8ed30364f10c951b",
      "28822e57532a42c0923bea868681f340",
      "15f0070dfd3e4f98a9ee02f6e6409bc6",
      "79c817f57f814cb9825bb2d2afff1dc2"
     ]
    },
    "id": "AJyo9c8SzxlW",
    "outputId": "3a765971-67d8-40c3-ec6f-7cf42fe6e02d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6f951883ce43bebbb76640b0eaa8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1bf1694ddf32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "RobertaConfig.from_pretrained('roberta-base')\n",
    "config.num_hidden_layers = 18\n",
    "config.num_attention_heads = 24\n",
    "\n",
    "\n",
    "model = RobertaPCL.from_pretrained('roberta-base',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RXeLZPRAZr40"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch import optim\n",
    "\n",
    "def train(trial, model, training_set, val_set, tokenizer):\n",
    "    \n",
    "\n",
    "    train_dataset = CustomDataset(tokenizer, training_set, include_community=True)\n",
    "    val_dataset = CustomDataset(tokenizer, val_set, include_community=True)\n",
    "\n",
    "    learning_rate_curve = CustomLossCallback(val_dataset)\n",
    "    early_stopping_callback = transformers.EarlyStoppingCallback(early_stopping_patience=4, early_stopping_threshold=0.0)\n",
    "\n",
    "\n",
    "    num_epochs = 2\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 7e-5)\n",
    "    \n",
    "    # our training args\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir='/',\n",
    "    logging_steps=200,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    #warmup_steps=100,\n",
    "    evaluation_strategy = 'steps',\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end = True\n",
    "    #warmup_ratio = 0.1,\n",
    "    #warmup_steps = int(num_training_steps * 0.1),\n",
    "    #optimizer_type = 'adamw'\n",
    "    )\n",
    "\n",
    "    # the trainer\n",
    "    trainer = TrainerCustom(\n",
    "    model=model,                         \n",
    "    args=training_args,                 \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,   \n",
    "    compute_metrics=compute_metrics,            \n",
    "    data_collator=train_dataset.collate_fn,\n",
    "    optimizers=(AdamW(model.parameters(), lr=lr), None),\n",
    "    callbacks=[learning_rate_curve, early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer,learning_rate_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "XXUx7SrfcLAX"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "    config.num_hidden_layers = trial.suggest_int('hidden layers', 9, 12)\n",
    "    config.num_attention_heads = trial.suggest_categorical('attention heads',[12,16])\n",
    "\n",
    "\n",
    "    model = RobertaCustom.from_pretrained('roberta-base',config=config)\n",
    "    model.cuda()\n",
    "    trainer,learning_rate_curve = train(trial, model, training_set, val_set, tokenizer)\n",
    "    evaluations = trainer.evaluate()\n",
    "    score = evaluations['eval_f1']\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yyzL1hhuoml2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 13:57:13,878]\u001b[0m A new study created in memory with name: no-name-c69ed702-28d7-4094-b7d4-d29aefa4948e\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d847a0512d85450f837f3333b706b337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1541\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1541' max='1541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1541/1541 02:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.346024</td>\n",
       "      <td>0.882259</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.108434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.353900</td>\n",
       "      <td>0.379634</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.178771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.528922</td>\n",
       "      <td>0.824980</td>\n",
       "      <td>0.359589</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.488372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.293200</td>\n",
       "      <td>0.293134</td>\n",
       "      <td>0.894193</td>\n",
       "      <td>0.525773</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.434043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.304200</td>\n",
       "      <td>0.320486</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.577586</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>0.527559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.433307</td>\n",
       "      <td>0.880668</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.509804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.239300</td>\n",
       "      <td>0.546986</td>\n",
       "      <td>0.871917</td>\n",
       "      <td>0.442786</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.525074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-200\n",
      "Configuration saved in /experiment/PCL/checkpoint-200/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-400\n",
      "Configuration saved in /experiment/PCL/checkpoint-400/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-600\n",
      "Configuration saved in /experiment/PCL/checkpoint-600/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-800\n",
      "Configuration saved in /experiment/PCL/checkpoint-800/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-1000\n",
      "Configuration saved in /experiment/PCL/checkpoint-1000/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-1200\n",
      "Configuration saved in /experiment/PCL/checkpoint-1200/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /experiment/PCL/checkpoint-1400\n",
      "Configuration saved in /experiment/PCL/checkpoint-1400/config.json\n",
      "Model weights saved in /experiment/PCL/checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /experiment/PCL/checkpoint-1000 (score: 0.5275590551181102).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 13:34:39,242]\u001b[0m Trial 0 finished with value: 0.5275590551181102 and parameters: {'hidden layers': 9, 'attention heads': 12, 'lr': 1.3785370313304477e-05}. Best is trial 0 with value: 0.5275590551181102.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study.optimize(objective, n_trials=50,timeout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "Kg8Jq40OKVeX",
    "outputId": "f49d4700-69fb-419d-d1a5-84ff2b177c3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_attention heads</th>\n",
       "      <th>params_hidden layers</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.527559</td>\n",
       "      <td>2023-03-04 13:31:45.455403</td>\n",
       "      <td>2023-03-04 13:34:39.241950</td>\n",
       "      <td>0 days 00:02:53.786547</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value             datetime_start          datetime_complete  \\\n",
       "0       0  0.527559 2023-03-04 13:31:45.455403 2023-03-04 13:34:39.241950   \n",
       "\n",
       "                duration  params_attention heads  params_hidden layers  \\\n",
       "0 0 days 00:02:53.786547                      12                     9   \n",
       "\n",
       "   params_lr     state  \n",
       "0   0.000014  COMPLETE  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "EgLvpuNh7GWv"
   },
   "outputs": [],
   "source": [
    "if os.path.exists('drive/MyDrive/NLPClassification_40/experiment/PCL/optuna.csv'):\n",
    "        # If the file exists, remove it\n",
    "        os.remove('drive/MyDrive/NLPClassification_40/experiment/PCL/optuna.csv')\n",
    " # Check if the file already exists\n",
    "if os.path.exists('drive/MyDrive/NLPClassification_40/experiment/PCL/optuna_study.pkl'):\n",
    "        # If the file exists, remove it\n",
    "        os.remove('drive/MyDrive/NLPClassification_40/experiment/PCL/optuna_study.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "1PMAlRh7TC3E",
    "outputId": "8125bd1e-25da-40bb-fe09-8b4ae14f2933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 0\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:09 < 03:48, 7.36 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.510900</td>\n",
       "      <td>0.357425</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.105960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.329709</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.323714</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.445087</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.495177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.280236</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.439716</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.360311</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.340580</td>\n",
       "      <td>0.431193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.326000</td>\n",
       "      <td>0.358357</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.492754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.592819</td>\n",
       "      <td>0.836118</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.495098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.49517684887459806).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:00:37,522]\u001b[0m Trial 0 finished with value: 0.49517684887459806 and parameters: {'hidden layers': 10, 'attention heads': 16, 'lr': 2.549303602197463e-05}. Best is trial 0 with value: 0.49517684887459806.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 06:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.475394</td>\n",
       "      <td>0.848051</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.173160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.409046</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.251208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.391127</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.454259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.398100</td>\n",
       "      <td>0.367617</td>\n",
       "      <td>0.880668</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.350914</td>\n",
       "      <td>0.884646</td>\n",
       "      <td>0.471545</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.305283</td>\n",
       "      <td>0.874304</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.455172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.384582</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.448485</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.488449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.340700</td>\n",
       "      <td>0.382951</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.485294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.290200</td>\n",
       "      <td>0.717242</td>\n",
       "      <td>0.813047</td>\n",
       "      <td>0.347003</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.483516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.392312</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.438710</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.464164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.396875</td>\n",
       "      <td>0.853620</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.525773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.467990</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.490066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.393423</td>\n",
       "      <td>0.886237</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.379098</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.509934</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.532872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.444784</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.510791</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.512635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2800 (score: 0.5328719723183393).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:07:07,159]\u001b[0m Trial 1 finished with value: 0.5328719723183393 and parameters: {'hidden layers': 10, 'attention heads': 12, 'lr': 3.9128351298540885e-05}. Best is trial 1 with value: 0.5328719723183393.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2800' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2800/3082 06:21 < 00:38, 7.34 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.405269</td>\n",
       "      <td>0.884646</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.026846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.523000</td>\n",
       "      <td>0.453745</td>\n",
       "      <td>0.886237</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.027211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.324086</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.369714</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.252427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.432600</td>\n",
       "      <td>0.570142</td>\n",
       "      <td>0.843278</td>\n",
       "      <td>0.348718</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.408408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.415865</td>\n",
       "      <td>0.857597</td>\n",
       "      <td>0.400966</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.481159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.673835</td>\n",
       "      <td>0.832936</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.449896</td>\n",
       "      <td>0.871917</td>\n",
       "      <td>0.439153</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.507645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.603440</td>\n",
       "      <td>0.827367</td>\n",
       "      <td>0.350943</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.397114</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.463687</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.523659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.325200</td>\n",
       "      <td>0.703753</td>\n",
       "      <td>0.833731</td>\n",
       "      <td>0.369963</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.491484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.401368</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.498084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.438220</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.506024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.381377</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.503497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2000 (score: 0.5236593059936907).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:13:33,405]\u001b[0m Trial 2 finished with value: 0.5236593059936907 and parameters: {'hidden layers': 10, 'attention heads': 16, 'lr': 4.241757409915724e-05}. Best is trial 1 with value: 0.5328719723183393.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 1\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/3082 03:38 < 03:22, 7.31 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.602600</td>\n",
       "      <td>0.612807</td>\n",
       "      <td>0.357995</td>\n",
       "      <td>0.142246</td>\n",
       "      <td>0.963768</td>\n",
       "      <td>0.247903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.400150</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.137681</td>\n",
       "      <td>0.205405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.318524</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.482446</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.311419</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.421546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.426192</td>\n",
       "      <td>0.891806</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>0.517968</td>\n",
       "      <td>0.864757</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.341085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.372214</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.415900</td>\n",
       "      <td>0.427341</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0.373984</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.352490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-800 (score: 0.42154566744730676).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:17:17,119]\u001b[0m Trial 3 finished with value: 0.42154566744730676 and parameters: {'hidden layers': 10, 'attention heads': 16, 'lr': 6.793568605406308e-05}. Best is trial 1 with value: 0.5328719723183393.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:05 < 03:43, 7.54 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.298794</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.351852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.365277</td>\n",
       "      <td>0.872713</td>\n",
       "      <td>0.438202</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.493671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.316315</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.454106</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.544928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.293183</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.540351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.304600</td>\n",
       "      <td>0.327573</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.514851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.354564</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.532423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.496287</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.543046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.5449275362318841).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:20:28,045]\u001b[0m Trial 4 finished with value: 0.5449275362318841 and parameters: {'hidden layers': 10, 'attention heads': 12, 'lr': 2.074304675388759e-05}. Best is trial 4 with value: 0.5449275362318841.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 04:56 < 01:24, 8.08 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>0.373041</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.028986</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.367329</td>\n",
       "      <td>0.856802</td>\n",
       "      <td>0.377907</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.380600</td>\n",
       "      <td>0.296239</td>\n",
       "      <td>0.882259</td>\n",
       "      <td>0.456897</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.417323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.286750</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.452555</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.450909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.352800</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.894193</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.340580</td>\n",
       "      <td>0.414097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.317200</td>\n",
       "      <td>0.418445</td>\n",
       "      <td>0.891806</td>\n",
       "      <td>0.508772</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.460317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.471267</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.322500</td>\n",
       "      <td>0.385055</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.486301</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.312700</td>\n",
       "      <td>0.947592</td>\n",
       "      <td>0.773270</td>\n",
       "      <td>0.302949</td>\n",
       "      <td>0.818841</td>\n",
       "      <td>0.442270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.381182</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.488189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.803250</td>\n",
       "      <td>0.833731</td>\n",
       "      <td>0.369004</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.488998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.669914</td>\n",
       "      <td>0.855211</td>\n",
       "      <td>0.401786</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.497238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:25:29,812]\u001b[0m Trial 5 finished with value: 0.5 and parameters: {'hidden layers': 9, 'attention heads': 16, 'lr': 1.241838529731025e-05}. Best is trial 4 with value: 0.5449275362318841.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 2\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 06:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.378767</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0.369748</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.342412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.384057</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.338164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.351098</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.394495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.372800</td>\n",
       "      <td>0.327713</td>\n",
       "      <td>0.880668</td>\n",
       "      <td>0.449153</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.414062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.318366</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.210145</td>\n",
       "      <td>0.315217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.321202</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.447059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.638208</td>\n",
       "      <td>0.834527</td>\n",
       "      <td>0.366412</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.338700</td>\n",
       "      <td>0.544741</td>\n",
       "      <td>0.850438</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.471910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.907072</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.450396</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.477612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.258300</td>\n",
       "      <td>0.471116</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.511811</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.490566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.244500</td>\n",
       "      <td>0.518324</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.477612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.417035</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.446764</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.437722</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.492857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2800 (score: 0.5106382978723404).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:32:27,703]\u001b[0m Trial 6 finished with value: 0.5106382978723404 and parameters: {'hidden layers': 10, 'attention heads': 16, 'lr': 4.037912118208743e-05}. Best is trial 4 with value: 0.5449275362318841.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 06:18, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>0.341263</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.446100</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.870326</td>\n",
       "      <td>0.398374</td>\n",
       "      <td>0.355072</td>\n",
       "      <td>0.375479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.398315</td>\n",
       "      <td>0.879077</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.410853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.353722</td>\n",
       "      <td>0.867144</td>\n",
       "      <td>0.414201</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.456026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.333800</td>\n",
       "      <td>0.339769</td>\n",
       "      <td>0.880668</td>\n",
       "      <td>0.457746</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.433215</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.538177</td>\n",
       "      <td>0.849642</td>\n",
       "      <td>0.381395</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.464589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.471751</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.433673</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.508982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.604716</td>\n",
       "      <td>0.813047</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.496983</td>\n",
       "      <td>0.881464</td>\n",
       "      <td>0.467066</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.511475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.491892</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.416268</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.501441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.448843</td>\n",
       "      <td>0.886237</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.437586</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.536232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.454309</td>\n",
       "      <td>0.886237</td>\n",
       "      <td>0.484472</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.464979</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.528571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2600 (score: 0.5362318840579711).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:38:51,258]\u001b[0m Trial 7 finished with value: 0.5362318840579711 and parameters: {'hidden layers': 9, 'attention heads': 16, 'lr': 5.030747578246703e-05}. Best is trial 4 with value: 0.5449275362318841.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.391234</td>\n",
       "      <td>0.882259</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.119048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.346705</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.493827</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.365297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.311751</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.475936</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.547692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.350770</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.471074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.412739</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.392157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.461933</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.529932</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.517801</td>\n",
       "      <td>0.886237</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.557276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.173400</td>\n",
       "      <td>0.669127</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.542373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.450267</td>\n",
       "      <td>0.910103</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.560311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.550974</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.566308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.126400</td>\n",
       "      <td>0.585666</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.557823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.518724</td>\n",
       "      <td>0.907717</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.573529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.520312</td>\n",
       "      <td>0.910899</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.594203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.525708</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.572491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2800 (score: 0.5942028985507246).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:46:06,006]\u001b[0m Trial 8 finished with value: 0.5942028985507246 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 3.147850151145127e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 3\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:02 < 03:39, 7.65 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.502400</td>\n",
       "      <td>0.382818</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.265306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.388587</td>\n",
       "      <td>0.834527</td>\n",
       "      <td>0.349138</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.437838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.341806</td>\n",
       "      <td>0.872713</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.540230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.316948</td>\n",
       "      <td>0.881464</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.050725</td>\n",
       "      <td>0.085890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.383484</td>\n",
       "      <td>0.902148</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.369231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.412729</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.458065</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.484642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.282700</td>\n",
       "      <td>0.561671</td>\n",
       "      <td>0.874304</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.487013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.5402298850574713).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:49:13,615]\u001b[0m Trial 9 finished with value: 0.5402298850574713 and parameters: {'hidden layers': 10, 'attention heads': 12, 'lr': 3.9488492811684736e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:34 < 04:18, 6.51 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.332117</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.378855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.274702</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.418410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.548495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.389276</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.510949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.305245</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.297101</td>\n",
       "      <td>0.416244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.408614</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.530201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.229100</td>\n",
       "      <td>0.516515</td>\n",
       "      <td>0.872713</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.540230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.548494983277592).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:52:54,010]\u001b[0m Trial 10 finished with value: 0.548494983277592 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.4462132789874935e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:49, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.412933</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.297101</td>\n",
       "      <td>0.379630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.462200</td>\n",
       "      <td>0.343128</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>0.264550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.443510</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>0.465278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.346628</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.525862</td>\n",
       "      <td>0.442029</td>\n",
       "      <td>0.480315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.380143</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.514563</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.439834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.442227</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.501629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.295200</td>\n",
       "      <td>0.409562</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.536424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.449575</td>\n",
       "      <td>0.867144</td>\n",
       "      <td>0.433790</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.532213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.620533</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>0.422131</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.539267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.446505</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.477876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.204400</td>\n",
       "      <td>0.471287</td>\n",
       "      <td>0.907717</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.539683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.475106</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.562963</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.556777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.495424</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.502994</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.550820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.528369</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.496894</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.535117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.572532</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.482558</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.535484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5567765567765567).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:00:49,000]\u001b[0m Trial 11 finished with value: 0.5567765567765567 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.6311446651808695e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 4\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3082 07:41 < 00:12, 6.50 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.465700</td>\n",
       "      <td>0.281636</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>0.356917</td>\n",
       "      <td>0.883850</td>\n",
       "      <td>0.473333</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.493056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.336500</td>\n",
       "      <td>0.399052</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.470948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>0.378039</td>\n",
       "      <td>0.853620</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.491713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.268359</td>\n",
       "      <td>0.906921</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.440191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.264000</td>\n",
       "      <td>0.367342</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.653148</td>\n",
       "      <td>0.870326</td>\n",
       "      <td>0.443439</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.545961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.399503</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.547619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.787429</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.427386</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.543536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136200</td>\n",
       "      <td>0.527789</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.468468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.565122</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.577181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.560212</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.511765</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.564935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.577445</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.535484</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.566553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.568588</td>\n",
       "      <td>0.902148</td>\n",
       "      <td>0.554745</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.552727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.563628</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.563910</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.553506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2200 (score: 0.5771812080536913).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:08:36,256]\u001b[0m Trial 12 finished with value: 0.5771812080536913 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.9698010429265052e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.494700</td>\n",
       "      <td>0.328210</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.123457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.397348</td>\n",
       "      <td>0.827367</td>\n",
       "      <td>0.362369</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.489412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.311300</td>\n",
       "      <td>0.436505</td>\n",
       "      <td>0.850438</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.502646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.353047</td>\n",
       "      <td>0.894193</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.512821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.433212</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.318365</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.529851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.477147</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>0.443722</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.572464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>0.747918</td>\n",
       "      <td>0.845664</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.519802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.148100</td>\n",
       "      <td>0.486568</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.539007</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.544803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.622423</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.502924</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.556634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.593957</td>\n",
       "      <td>0.907717</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.573529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.583489</td>\n",
       "      <td>0.900557</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.096400</td>\n",
       "      <td>0.595560</td>\n",
       "      <td>0.900557</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.555160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.596442</td>\n",
       "      <td>0.906921</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.561798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5735294117647058).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:15:57,188]\u001b[0m Trial 13 finished with value: 0.5735294117647058 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 3.2709595777577935e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/3082 04:42 < 02:32, 7.08 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.302265</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.426966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.351200</td>\n",
       "      <td>0.513109</td>\n",
       "      <td>0.823389</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.463768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.342911</td>\n",
       "      <td>0.876691</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.523077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.355724</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.355072</td>\n",
       "      <td>0.417021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.331516</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.547445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.363249</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.562963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.710761</td>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.530806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.240100</td>\n",
       "      <td>0.482833</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.464455</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.561605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.710651</td>\n",
       "      <td>0.838504</td>\n",
       "      <td>0.388316</td>\n",
       "      <td>0.818841</td>\n",
       "      <td>0.526807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.174300</td>\n",
       "      <td>0.500838</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.581967</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.546154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1200 (score: 0.562962962962963).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:20:44,809]\u001b[0m Trial 14 finished with value: 0.562962962962963 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 1.0837245917639206e-05}. Best is trial 8 with value: 0.5942028985507246.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 5\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.357541</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>0.103226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.375700</td>\n",
       "      <td>0.354390</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.254144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.489923</td>\n",
       "      <td>0.809069</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.418204</td>\n",
       "      <td>0.873508</td>\n",
       "      <td>0.445596</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.519637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.380529</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.469027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.309751</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.471014</td>\n",
       "      <td>0.517928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.413269</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.536424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.210300</td>\n",
       "      <td>0.412376</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.506438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>0.528432</td>\n",
       "      <td>0.880668</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.556213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>0.465597</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.551471</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.547445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.523029</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.601770</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.541833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.553739</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.532051</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.564626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>0.554260</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.518717</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.596923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.553456</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.546053</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.572414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.556628</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.572581</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.541985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2600 (score: 0.5969230769230769).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:28:33,103]\u001b[0m Trial 15 finished with value: 0.5969230769230769 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 3.272626949305586e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 05:37 < 01:36, 7.10 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>0.313714</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.268116</td>\n",
       "      <td>0.339450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.351300</td>\n",
       "      <td>0.312491</td>\n",
       "      <td>0.856802</td>\n",
       "      <td>0.399038</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.479769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.337321</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.421525</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.520776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.304383</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.546053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.379519</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.466368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.360928</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>0.557778</td>\n",
       "      <td>0.853620</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.492971</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.492147</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.724114</td>\n",
       "      <td>0.867940</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.550770</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.469027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.681068</td>\n",
       "      <td>0.883850</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.562874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.655399</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.497076</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.550162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5714285714285714).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:34:15,918]\u001b[0m Trial 16 finished with value: 0.5714285714285714 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 1.761338588969213e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 06:06 < 01:44, 6.55 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>0.393026</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.041379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.423884</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.413043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.406700</td>\n",
       "      <td>0.438550</td>\n",
       "      <td>0.861575</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.472727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.330835</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.268116</td>\n",
       "      <td>0.377551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.378768</td>\n",
       "      <td>0.875895</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.486842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.282400</td>\n",
       "      <td>0.365387</td>\n",
       "      <td>0.883850</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.522876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.536202</td>\n",
       "      <td>0.870326</td>\n",
       "      <td>0.434555</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.504559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.442503</td>\n",
       "      <td>0.884646</td>\n",
       "      <td>0.482927</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.577259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.662312</td>\n",
       "      <td>0.866348</td>\n",
       "      <td>0.430556</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.525424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.451124</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.528986</td>\n",
       "      <td>0.534799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.498051</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.519481</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.547945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>0.491307</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.542254</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5772594752186588).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:40:27,747]\u001b[0m Trial 17 finished with value: 0.5772594752186588 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 3.281036929148478e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 6\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.543300</td>\n",
       "      <td>0.290310</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.450382</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.438662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.364900</td>\n",
       "      <td>0.349446</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.434483</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.445230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.375926</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.456044</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.518750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.284519</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.396013</td>\n",
       "      <td>0.900557</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.423963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.395725</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.522293</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.555932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.242100</td>\n",
       "      <td>0.599238</td>\n",
       "      <td>0.861575</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.524590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.212100</td>\n",
       "      <td>0.541500</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.442982</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.551913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.661472</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.446281</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.568421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.483599</td>\n",
       "      <td>0.902148</td>\n",
       "      <td>0.549669</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.574394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.604746</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.524096</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.572368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.573488</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.584416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.539084</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.594771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.578739</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.565517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.588039</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.557047</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.578397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2600 (score: 0.5947712418300652).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:47:46,031]\u001b[0m Trial 18 finished with value: 0.5947712418300652 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 1.8608894385655586e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 06:05 < 01:43, 6.56 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.376814</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.354947</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.517857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.454219</td>\n",
       "      <td>0.868735</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.537815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.298300</td>\n",
       "      <td>0.362050</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.440367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.306283</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.427861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.253900</td>\n",
       "      <td>0.321203</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.598361</td>\n",
       "      <td>0.528986</td>\n",
       "      <td>0.561538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.242700</td>\n",
       "      <td>0.590090</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0.435685</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.554090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.466492</td>\n",
       "      <td>0.902148</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.588629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174400</td>\n",
       "      <td>0.702085</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.430279</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.555270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>0.522697</td>\n",
       "      <td>0.911695</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.564706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.610369</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.526012</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.585209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.624956</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.526627</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.579805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5886287625418061).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 15:53:57,290]\u001b[0m Trial 19 finished with value: 0.5886287625418061 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 1.7110882444647594e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.554800</td>\n",
       "      <td>0.285401</td>\n",
       "      <td>0.868735</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.408602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.307293</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.414634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.355761</td>\n",
       "      <td>0.883055</td>\n",
       "      <td>0.470199</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.491349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.336295</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.317100</td>\n",
       "      <td>0.450723</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.378641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.316148</td>\n",
       "      <td>0.888624</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.521219</td>\n",
       "      <td>0.874304</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.545977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.462438</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.521212</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.567657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.766124</td>\n",
       "      <td>0.863166</td>\n",
       "      <td>0.435115</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.576252</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.616279</td>\n",
       "      <td>0.384058</td>\n",
       "      <td>0.473214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.605969</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.529851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.636029</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>0.533307</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.612412</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.628782</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.524138</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.537102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5786163522012578).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:01:13,330]\u001b[0m Trial 20 finished with value: 0.5786163522012578 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 2.053071363610748e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 7\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.284960</td>\n",
       "      <td>0.881464</td>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.293839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.298306</td>\n",
       "      <td>0.891806</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.181159</td>\n",
       "      <td>0.268817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.505305</td>\n",
       "      <td>0.842482</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.502513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.362621</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.414747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.340900</td>\n",
       "      <td>0.387979</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.411215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.288200</td>\n",
       "      <td>0.542239</td>\n",
       "      <td>0.875895</td>\n",
       "      <td>0.450549</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.582758</td>\n",
       "      <td>0.877486</td>\n",
       "      <td>0.454023</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.506410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.422812</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.502857</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.203400</td>\n",
       "      <td>0.575362</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.576471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.422773</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.581470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.487445</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.565217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.484166</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.587413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.501470</td>\n",
       "      <td>0.910103</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.567050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.537585</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.558621</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.572438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>0.541137</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.568493</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.584507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5874125874125874).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:08:29,507]\u001b[0m Trial 21 finished with value: 0.5874125874125874 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 2.9813949979331525e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 05:38 < 01:36, 7.08 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.301541</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.402235</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.454259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.406510</td>\n",
       "      <td>0.849642</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.473538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.319400</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>0.843278</td>\n",
       "      <td>0.384314</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.498728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.282100</td>\n",
       "      <td>0.380732</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.471483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.390301</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.441558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.355468</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.529183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.582777</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.427386</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.543536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.482881</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.582353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>0.630277</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.492228</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.574018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.570792</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.535948</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.563574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.603656</td>\n",
       "      <td>0.898170</td>\n",
       "      <td>0.532895</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.558621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.599191</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5823529411764705).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:14:13,520]\u001b[0m Trial 22 finished with value: 0.5823529411764705 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 1.4428819497382392e-05}. Best is trial 15 with value: 0.5969230769230769.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.280034</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.471483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.318064</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.268116</td>\n",
       "      <td>0.360976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.326100</td>\n",
       "      <td>0.369334</td>\n",
       "      <td>0.881464</td>\n",
       "      <td>0.470270</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.538700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.325580</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.298500</td>\n",
       "      <td>0.321431</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.343074</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.523985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>0.524289</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.483146</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.544304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.443983</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.566176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>0.619718</td>\n",
       "      <td>0.871917</td>\n",
       "      <td>0.448889</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.556474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.487413</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.513274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.543624</td>\n",
       "      <td>0.907717</td>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.546875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.597315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.561384</td>\n",
       "      <td>0.903739</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.575439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.545523</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.566434</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.576512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.551722</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.557971</td>\n",
       "      <td>0.574627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5973154362416107).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:22:06,087]\u001b[0m Trial 23 finished with value: 0.5973154362416107 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.2343278770417364e-05}. Best is trial 23 with value: 0.5973154362416107.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 8\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 06:10 < 01:45, 6.47 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.294468</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.326087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.372830</td>\n",
       "      <td>0.900557</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.365482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>0.431381</td>\n",
       "      <td>0.870326</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.530259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.281700</td>\n",
       "      <td>0.398131</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.502165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.353942</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.319938</td>\n",
       "      <td>0.910899</td>\n",
       "      <td>0.616071</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.220100</td>\n",
       "      <td>0.582064</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.442478</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.549451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.211100</td>\n",
       "      <td>0.476019</td>\n",
       "      <td>0.897375</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.574257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.731580</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.496137</td>\n",
       "      <td>0.913286</td>\n",
       "      <td>0.628319</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.565737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.627334</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.496970</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.541254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.610261</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.503145</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.538721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5742574257425742).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:28:22,397]\u001b[0m Trial 24 finished with value: 0.5742574257425742 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.122029511392304e-05}. Best is trial 23 with value: 0.5973154362416107.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>0.308472</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.323529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.337100</td>\n",
       "      <td>0.328289</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.389831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.280900</td>\n",
       "      <td>0.450690</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.528736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.343879</td>\n",
       "      <td>0.901352</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.465517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.485917</td>\n",
       "      <td>0.904535</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.340659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.296782</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.508621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>0.627463</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.534435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.212700</td>\n",
       "      <td>0.513698</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.565789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.775517</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.443478</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.554348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.581911</td>\n",
       "      <td>0.898966</td>\n",
       "      <td>0.551402</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.481633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.599965</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.560886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.653244</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.522472</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.588608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.085800</td>\n",
       "      <td>0.653512</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.565789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.624579</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.551724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.636877</td>\n",
       "      <td>0.899761</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.553191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-2400 (score: 0.5886075949367089).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:36:19,879]\u001b[0m Trial 25 finished with value: 0.5886075949367089 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 1.57546408772339e-05}. Best is trial 23 with value: 0.5973154362416107.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:33 < 04:17, 6.54 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.314531</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.268116</td>\n",
       "      <td>0.354067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.343942</td>\n",
       "      <td>0.887033</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.413223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.491369</td>\n",
       "      <td>0.863166</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.544974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.331904</td>\n",
       "      <td>0.902944</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.495868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.299813</td>\n",
       "      <td>0.908512</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.502165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.358740</td>\n",
       "      <td>0.907717</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.485507</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.577522</td>\n",
       "      <td>0.865553</td>\n",
       "      <td>0.429224</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.526611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.5449735449735449).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:39:59,340]\u001b[0m Trial 26 finished with value: 0.5449735449735449 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 1.179513695615018e-05}. Best is trial 23 with value: 0.5973154362416107.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "I: 9\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3082 03:35 < 04:19, 6.49 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.299127</td>\n",
       "      <td>0.884646</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.137681</td>\n",
       "      <td>0.207650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.349400</td>\n",
       "      <td>0.602467</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.332308</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.466523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.318620</td>\n",
       "      <td>0.894193</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.555184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.360733</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.481707</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.523179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.480886</td>\n",
       "      <td>0.906921</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.353591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.262600</td>\n",
       "      <td>0.353442</td>\n",
       "      <td>0.912490</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.460814</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.467662</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.554572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-600 (score: 0.5551839464882943).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:43:40,802]\u001b[0m Trial 27 finished with value: 0.5551839464882943 and parameters: {'hidden layers': 12, 'attention heads': 12, 'lr': 2.2457475658391383e-05}. Best is trial 23 with value: 0.5973154362416107.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2400' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2400/3082 05:39 < 01:36, 7.06 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.359811</td>\n",
       "      <td>0.891010</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.311558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.386641</td>\n",
       "      <td>0.853620</td>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.455893</td>\n",
       "      <td>0.856006</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.524934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.324077</td>\n",
       "      <td>0.889419</td>\n",
       "      <td>0.496644</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.515679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.333258</td>\n",
       "      <td>0.906126</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>0.420290</td>\n",
       "      <td>0.495726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>0.392187</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.565789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.631813</td>\n",
       "      <td>0.869531</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.544444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.545792</td>\n",
       "      <td>0.894988</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.648981</td>\n",
       "      <td>0.879873</td>\n",
       "      <td>0.467980</td>\n",
       "      <td>0.688406</td>\n",
       "      <td>0.557185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.517950</td>\n",
       "      <td>0.910899</td>\n",
       "      <td>0.635417</td>\n",
       "      <td>0.442029</td>\n",
       "      <td>0.521368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.115400</td>\n",
       "      <td>0.605220</td>\n",
       "      <td>0.890215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.644928</td>\n",
       "      <td>0.563291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.590760</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.659420</td>\n",
       "      <td>0.575949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-1600 (score: 0.5999999999999999).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:49:26,107]\u001b[0m Trial 28 finished with value: 0.5999999999999999 and parameters: {'hidden layers': 11, 'attention heads': 12, 'lr': 1.667570074122472e-05}. Best is trial 28 with value: 0.5999999999999999.\u001b[0m\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaCustom: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaCustom from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaCustom from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaCustom were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.1.weight', 'classifier.4.bias', 'classifier.4.weight', 'roberta.embeddings.position_ids', 'classifier.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 12321\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3082\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3082' max='3082' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3082/3082 07:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.498800</td>\n",
       "      <td>0.338505</td>\n",
       "      <td>0.782816</td>\n",
       "      <td>0.298507</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.422833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.364542</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.314286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.408800</td>\n",
       "      <td>0.525482</td>\n",
       "      <td>0.821002</td>\n",
       "      <td>0.344086</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.460432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.817820</td>\n",
       "      <td>0.304721</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.382749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>0.310567</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.340580</td>\n",
       "      <td>0.419643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.361943</td>\n",
       "      <td>0.892601</td>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.427536</td>\n",
       "      <td>0.466403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.359362</td>\n",
       "      <td>0.862371</td>\n",
       "      <td>0.403315</td>\n",
       "      <td>0.528986</td>\n",
       "      <td>0.457680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.361972</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.526667</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.548611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.289100</td>\n",
       "      <td>0.721472</td>\n",
       "      <td>0.817025</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.768116</td>\n",
       "      <td>0.479638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.393985</td>\n",
       "      <td>0.896579</td>\n",
       "      <td>0.528169</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.456452</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.491620</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.555205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.240200</td>\n",
       "      <td>0.483072</td>\n",
       "      <td>0.893397</td>\n",
       "      <td>0.512346</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.553333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.456971</td>\n",
       "      <td>0.885442</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.423034</td>\n",
       "      <td>0.894193</td>\n",
       "      <td>0.515723</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.552189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.442249</td>\n",
       "      <td>0.895784</td>\n",
       "      <td>0.522293</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.555932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-200\n",
      "Configuration saved in /checkpoint-200/config.json\n",
      "Model weights saved in /checkpoint-200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-400\n",
      "Configuration saved in /checkpoint-400/config.json\n",
      "Model weights saved in /checkpoint-400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-600\n",
      "Configuration saved in /checkpoint-600/config.json\n",
      "Model weights saved in /checkpoint-600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-800\n",
      "Configuration saved in /checkpoint-800/config.json\n",
      "Model weights saved in /checkpoint-800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1000\n",
      "Configuration saved in /checkpoint-1000/config.json\n",
      "Model weights saved in /checkpoint-1000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1200\n",
      "Configuration saved in /checkpoint-1200/config.json\n",
      "Model weights saved in /checkpoint-1200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1400\n",
      "Configuration saved in /checkpoint-1400/config.json\n",
      "Model weights saved in /checkpoint-1400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1600\n",
      "Configuration saved in /checkpoint-1600/config.json\n",
      "Model weights saved in /checkpoint-1600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-1800\n",
      "Configuration saved in /checkpoint-1800/config.json\n",
      "Model weights saved in /checkpoint-1800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2000\n",
      "Configuration saved in /checkpoint-2000/config.json\n",
      "Model weights saved in /checkpoint-2000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2200\n",
      "Configuration saved in /checkpoint-2200/config.json\n",
      "Model weights saved in /checkpoint-2200/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2400\n",
      "Configuration saved in /checkpoint-2400/config.json\n",
      "Model weights saved in /checkpoint-2400/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2600\n",
      "Configuration saved in /checkpoint-2600/config.json\n",
      "Model weights saved in /checkpoint-2600/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-2800\n",
      "Configuration saved in /checkpoint-2800/config.json\n",
      "Model weights saved in /checkpoint-2800/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /checkpoint-3000\n",
      "Configuration saved in /checkpoint-3000/config.json\n",
      "Model weights saved in /checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /checkpoint-3000 (score: 0.5559322033898305).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1257\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='158' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [158/158 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 16:57:24,775]\u001b[0m Trial 29 finished with value: 0.5559322033898305 and parameters: {'hidden layers': 12, 'attention heads': 16, 'lr': 2.5456546603509103e-05}. Best is trial 28 with value: 0.5999999999999999.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#study = optuna.create_study(direction='maximize')\n",
    "for i in range(10):\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(f'I: {i}')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    \n",
    "    study.optimize(objective, n_trials=3,timeout=None)\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists('optuna_study.pkl'):\n",
    "        # If the file exists, remove it\n",
    "        os.remove('optuna_study.pkl')\n",
    "\n",
    "    with open('optuna_study.pkl', 'wb') as f:\n",
    "            pickle.dump(study, f)\n",
    "\n",
    "    # save the study in dataframe\n",
    "    d = study.trials_dataframe()\n",
    "    # reset index\n",
    "    d.reset_index(inplace=True,drop=True)\n",
    "    # save the dataframe\n",
    "        # Check if the file already exists\n",
    "    if os.path.exists('optuna.csv'):\n",
    "        # If the file exists, remove it\n",
    "        os.remove('optuna.csv')\n",
    "\n",
    "    d.to_csv('optuna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u4dpvdXz9jj8",
    "outputId": "4d5195c2-adc4-43b0-a5df-4ba6305a31fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_attention heads</th>\n",
       "      <th>params_hidden layers</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.604230</td>\n",
       "      <td>2023-03-04 13:38:32.501270</td>\n",
       "      <td>2023-03-04 13:44:02.928866</td>\n",
       "      <td>0 days 00:05:30.427596</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-04 13:44:02.937565</td>\n",
       "      <td>2023-03-04 13:47:30.541651</td>\n",
       "      <td>0 days 00:03:27.604086</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-04 13:49:53.556013</td>\n",
       "      <td>2023-03-04 13:51:36.800269</td>\n",
       "      <td>0 days 00:01:43.244256</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-04 13:51:59.281339</td>\n",
       "      <td>2023-03-04 13:52:01.302142</td>\n",
       "      <td>0 days 00:00:02.020803</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>2023-03-04 13:52:41.302610</td>\n",
       "      <td>2023-03-04 13:55:46.617291</td>\n",
       "      <td>0 days 00:03:05.314681</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value             datetime_start          datetime_complete  \\\n",
       "0       0  0.604230 2023-03-04 13:38:32.501270 2023-03-04 13:44:02.928866   \n",
       "1       1       NaN 2023-03-04 13:44:02.937565 2023-03-04 13:47:30.541651   \n",
       "2       2       NaN 2023-03-04 13:49:53.556013 2023-03-04 13:51:36.800269   \n",
       "3       3       NaN 2023-03-04 13:51:59.281339 2023-03-04 13:52:01.302142   \n",
       "4       4  0.473684 2023-03-04 13:52:41.302610 2023-03-04 13:55:46.617291   \n",
       "\n",
       "                duration  params_attention heads  params_hidden layers  \\\n",
       "0 0 days 00:05:30.427596                      12                    11   \n",
       "1 0 days 00:03:27.604086                      12                     9   \n",
       "2 0 days 00:01:43.244256                      16                    11   \n",
       "3 0 days 00:00:02.020803                      16                    10   \n",
       "4 0 days 00:03:05.314681                      16                    11   \n",
       "\n",
       "   params_lr     state  \n",
       "0   0.000028  COMPLETE  \n",
       "1   0.000030      FAIL  \n",
       "2   0.000033      FAIL  \n",
       "3   0.000044      FAIL  \n",
       "4   0.000028  COMPLETE  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('/experiment/PCL/optuna_study.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/optuna'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECWF-SCpqb5r"
   },
   "source": [
    "**Load the study**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz_J83yAqeRR"
   },
   "outputs": [],
   "source": [
    "# Load the study from the file\n",
    "with open('/content/drive/MyDrive/NLPClassification_40/experiment/PCL/optuna_study.pkl', 'rb') as f:\n",
    "    study_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbM5jcFi5wIT"
   },
   "outputs": [],
   "source": [
    "study=study_loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uWtxYfRGKhVj",
    "outputId": "6341a520-fa5b-4240-85ea-58a9c0139692"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f2c2bdf5-7827-4a9d-921c-43920002c086\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_attention heads</th>\n",
       "      <th>params_hidden layers</th>\n",
       "      <th>params_lr</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.506224</td>\n",
       "      <td>2023-03-03 10:38:17.643929</td>\n",
       "      <td>2023-03-03 10:50:16.269986</td>\n",
       "      <td>0 days 00:11:58.626057</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.533865</td>\n",
       "      <td>2023-03-03 10:50:16.294265</td>\n",
       "      <td>2023-03-03 11:01:59.989614</td>\n",
       "      <td>0 days 00:11:43.695349</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>2023-03-03 11:02:00.043813</td>\n",
       "      <td>2023-03-03 11:12:01.809789</td>\n",
       "      <td>0 days 00:10:01.765976</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>2023-03-03 11:12:01.843374</td>\n",
       "      <td>2023-03-03 11:23:45.258922</td>\n",
       "      <td>0 days 00:11:43.415548</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.496350</td>\n",
       "      <td>2023-03-03 11:23:45.305154</td>\n",
       "      <td>2023-03-03 11:32:54.252477</td>\n",
       "      <td>0 days 00:09:08.947323</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>2023-03-03 11:32:54.310483</td>\n",
       "      <td>2023-03-03 11:42:58.468804</td>\n",
       "      <td>0 days 00:10:04.158321</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.502024</td>\n",
       "      <td>2023-03-03 11:42:58.511926</td>\n",
       "      <td>2023-03-03 11:52:54.184696</td>\n",
       "      <td>0 days 00:09:55.672770</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>2023-03-03 11:52:54.235457</td>\n",
       "      <td>2023-03-03 12:02:01.356595</td>\n",
       "      <td>0 days 00:09:07.121138</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.578512</td>\n",
       "      <td>2023-03-03 12:02:01.417602</td>\n",
       "      <td>2023-03-03 12:13:47.115379</td>\n",
       "      <td>0 days 00:11:45.697777</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>2023-03-03 12:13:47.171368</td>\n",
       "      <td>2023-03-03 12:25:32.574710</td>\n",
       "      <td>0 days 00:11:45.403342</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>2023-03-03 12:25:32.634605</td>\n",
       "      <td>2023-03-03 12:36:26.033570</td>\n",
       "      <td>0 days 00:10:53.398965</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>2023-03-03 12:36:26.092827</td>\n",
       "      <td>2023-03-03 12:47:17.945122</td>\n",
       "      <td>0 days 00:10:51.852295</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-03 12:47:17.992948</td>\n",
       "      <td>2023-03-03 12:47:59.751258</td>\n",
       "      <td>0 days 00:00:41.758310</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>2023-03-03 12:50:41.313899</td>\n",
       "      <td>2023-03-03 13:01:44.062950</td>\n",
       "      <td>0 days 00:11:02.749051</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.547529</td>\n",
       "      <td>2023-03-03 13:01:44.110447</td>\n",
       "      <td>2023-03-03 13:12:40.550268</td>\n",
       "      <td>0 days 00:10:56.439821</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>2023-03-03 13:12:40.597140</td>\n",
       "      <td>2023-03-03 13:23:32.499271</td>\n",
       "      <td>0 days 00:10:51.902131</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.570281</td>\n",
       "      <td>2023-03-03 13:23:32.548371</td>\n",
       "      <td>2023-03-03 13:34:29.202496</td>\n",
       "      <td>0 days 00:10:56.654125</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>2023-03-03 13:34:29.268139</td>\n",
       "      <td>2023-03-03 13:45:24.516065</td>\n",
       "      <td>0 days 00:10:55.247926</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.552448</td>\n",
       "      <td>2023-03-03 13:45:24.559854</td>\n",
       "      <td>2023-03-03 13:55:21.774457</td>\n",
       "      <td>0 days 00:09:57.214603</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>2023-03-03 13:55:21.829098</td>\n",
       "      <td>2023-03-03 14:06:23.263343</td>\n",
       "      <td>0 days 00:11:01.434245</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>2023-03-03 14:06:23.315037</td>\n",
       "      <td>2023-03-03 14:16:24.093671</td>\n",
       "      <td>0 days 00:10:00.778634</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>2023-03-03 14:16:24.149059</td>\n",
       "      <td>2023-03-03 14:27:18.344516</td>\n",
       "      <td>0 days 00:10:54.195457</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>2023-03-03 14:27:18.404426</td>\n",
       "      <td>2023-03-03 14:39:07.126190</td>\n",
       "      <td>0 days 00:11:48.721764</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.533865</td>\n",
       "      <td>2023-03-03 14:39:07.208414</td>\n",
       "      <td>2023-03-03 14:50:55.535633</td>\n",
       "      <td>0 days 00:11:48.327219</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>2023-03-03 14:50:55.590008</td>\n",
       "      <td>2023-03-03 15:01:46.944378</td>\n",
       "      <td>0 days 00:10:51.354370</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>2023-03-03 15:01:47.005131</td>\n",
       "      <td>2023-03-03 15:13:33.556937</td>\n",
       "      <td>0 days 00:11:46.551806</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.541833</td>\n",
       "      <td>2023-03-03 15:13:33.609565</td>\n",
       "      <td>2023-03-03 15:24:26.726961</td>\n",
       "      <td>0 days 00:10:53.117396</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>2023-03-03 15:24:26.802622</td>\n",
       "      <td>2023-03-03 15:36:22.596289</td>\n",
       "      <td>0 days 00:11:55.793667</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2c2bdf5-7827-4a9d-921c-43920002c086')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f2c2bdf5-7827-4a9d-921c-43920002c086 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f2c2bdf5-7827-4a9d-921c-43920002c086');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "0        0  0.506224 2023-03-03 10:38:17.643929 2023-03-03 10:50:16.269986   \n",
       "1        1  0.533865 2023-03-03 10:50:16.294265 2023-03-03 11:01:59.989614   \n",
       "2        2  0.467213 2023-03-03 11:02:00.043813 2023-03-03 11:12:01.809789   \n",
       "3        3  0.540541 2023-03-03 11:12:01.843374 2023-03-03 11:23:45.258922   \n",
       "4        4  0.496350 2023-03-03 11:23:45.305154 2023-03-03 11:32:54.252477   \n",
       "5        5  0.528302 2023-03-03 11:32:54.310483 2023-03-03 11:42:58.468804   \n",
       "6        6  0.502024 2023-03-03 11:42:58.511926 2023-03-03 11:52:54.184696   \n",
       "7        7  0.518519 2023-03-03 11:52:54.235457 2023-03-03 12:02:01.356595   \n",
       "8        8  0.578512 2023-03-03 12:02:01.417602 2023-03-03 12:13:47.115379   \n",
       "9        9  0.530120 2023-03-03 12:13:47.171368 2023-03-03 12:25:32.574710   \n",
       "10      10  0.539683 2023-03-03 12:25:32.634605 2023-03-03 12:36:26.033570   \n",
       "11      11  0.580645 2023-03-03 12:36:26.092827 2023-03-03 12:47:17.945122   \n",
       "12      12       NaN 2023-03-03 12:47:17.992948 2023-03-03 12:47:59.751258   \n",
       "13      13  0.560000 2023-03-03 12:50:41.313899 2023-03-03 13:01:44.062950   \n",
       "14      14  0.547529 2023-03-03 13:01:44.110447 2023-03-03 13:12:40.550268   \n",
       "15      15  0.584615 2023-03-03 13:12:40.597140 2023-03-03 13:23:32.499271   \n",
       "16      16  0.570281 2023-03-03 13:23:32.548371 2023-03-03 13:34:29.202496   \n",
       "17      17  0.565574 2023-03-03 13:34:29.268139 2023-03-03 13:45:24.516065   \n",
       "18      18  0.552448 2023-03-03 13:45:24.559854 2023-03-03 13:55:21.774457   \n",
       "19      19  0.528302 2023-03-03 13:55:21.829098 2023-03-03 14:06:23.263343   \n",
       "20      20  0.532819 2023-03-03 14:06:23.315037 2023-03-03 14:16:24.093671   \n",
       "21      21  0.511628 2023-03-03 14:16:24.149059 2023-03-03 14:27:18.344516   \n",
       "22      22  0.528455 2023-03-03 14:27:18.404426 2023-03-03 14:39:07.126190   \n",
       "23      23  0.533865 2023-03-03 14:39:07.208414 2023-03-03 14:50:55.535633   \n",
       "24      24  0.528455 2023-03-03 14:50:55.590008 2023-03-03 15:01:46.944378   \n",
       "25      25  0.554688 2023-03-03 15:01:47.005131 2023-03-03 15:13:33.556937   \n",
       "26      26  0.541833 2023-03-03 15:13:33.609565 2023-03-03 15:24:26.726961   \n",
       "27      27  0.479167 2023-03-03 15:24:26.802622 2023-03-03 15:36:22.596289   \n",
       "\n",
       "                 duration  params_attention heads  params_hidden layers  \\\n",
       "0  0 days 00:11:58.626057                      12                    12   \n",
       "1  0 days 00:11:43.695349                      12                    12   \n",
       "2  0 days 00:10:01.765976                      16                    10   \n",
       "3  0 days 00:11:43.415548                      12                    12   \n",
       "4  0 days 00:09:08.947323                      16                     9   \n",
       "5  0 days 00:10:04.158321                      16                    10   \n",
       "6  0 days 00:09:55.672770                      12                    10   \n",
       "7  0 days 00:09:07.121138                      16                     9   \n",
       "8  0 days 00:11:45.697777                      12                    12   \n",
       "9  0 days 00:11:45.403342                      12                    12   \n",
       "10 0 days 00:10:53.398965                      12                    11   \n",
       "11 0 days 00:10:51.852295                      12                    11   \n",
       "12 0 days 00:00:41.758310                      12                    11   \n",
       "13 0 days 00:11:02.749051                      12                    11   \n",
       "14 0 days 00:10:56.439821                      12                    11   \n",
       "15 0 days 00:10:51.902131                      12                    11   \n",
       "16 0 days 00:10:56.654125                      12                    11   \n",
       "17 0 days 00:10:55.247926                      12                    11   \n",
       "18 0 days 00:09:57.214603                      12                    10   \n",
       "19 0 days 00:11:01.434245                      16                    11   \n",
       "20 0 days 00:10:00.778634                      12                    10   \n",
       "21 0 days 00:10:54.195457                      12                    11   \n",
       "22 0 days 00:11:48.721764                      12                    12   \n",
       "23 0 days 00:11:48.327219                      12                    12   \n",
       "24 0 days 00:10:51.354370                      12                    11   \n",
       "25 0 days 00:11:46.551806                      12                    12   \n",
       "26 0 days 00:10:53.117396                      12                    11   \n",
       "27 0 days 00:11:55.793667                      16                    12   \n",
       "\n",
       "    params_lr     state  \n",
       "0    0.000034  COMPLETE  \n",
       "1    0.000034  COMPLETE  \n",
       "2    0.000032  COMPLETE  \n",
       "3    0.000032  COMPLETE  \n",
       "4    0.000039  COMPLETE  \n",
       "5    0.000037  COMPLETE  \n",
       "6    0.000039  COMPLETE  \n",
       "7    0.000038  COMPLETE  \n",
       "8    0.000030  COMPLETE  \n",
       "9    0.000030  COMPLETE  \n",
       "10   0.000030  COMPLETE  \n",
       "11   0.000032  COMPLETE  \n",
       "12   0.000032      FAIL  \n",
       "13   0.000032  COMPLETE  \n",
       "14   0.000033  COMPLETE  \n",
       "15   0.000031  COMPLETE  \n",
       "16   0.000034  COMPLETE  \n",
       "17   0.000031  COMPLETE  \n",
       "18   0.000033  COMPLETE  \n",
       "19   0.000035  COMPLETE  \n",
       "20   0.000031  COMPLETE  \n",
       "21   0.000032  COMPLETE  \n",
       "22   0.000031  COMPLETE  \n",
       "23   0.000031  COMPLETE  \n",
       "24   0.000031  COMPLETE  \n",
       "25   0.000030  COMPLETE  \n",
       "26   0.000032  COMPLETE  \n",
       "27   0.000031  COMPLETE  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_loaded.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QREhXw_nnOrt"
   },
   "source": [
    "### **Optuna END**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "74gyYUMF8_xJ",
    "outputId": "a0d861e2-7716-4cd5-a124-98b7789dd9fe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAugAAAHxCAYAAAA/a0leAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABYHklEQVR4nO3deXxU9b3/8feELCQEQoCEkMlGgmBA2WQXN8AuVC8KKFjWYu/11h+0WrWipddqUepWK2hbuaI1ilQ20VKhkrAlgmEVEAJigIQEiBgWCQkhIfP749yJmcxMNmYyJ8nr+XjMI+acM2e+J0cy73zncz7HYrPZbAIAAABgCn6+HgAAAACA7xHQAQAAABMhoAMAAAAmQkAHAAAATISADgAAAJgIAR0AAAAwEX9fD8BsvvjiCwUFBfnktUtLS3322mgcnOOWgfPcMnCemz/Occvgy/NcWlqqvn37Oi0noFcTFBSk5ORkn7x2VlaWz14bjYNz3DJwnlsGznPzxzluGXx5nrOyslwup8QFAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYiE9vVLR27Vpt375dWVlZOnjwoC5evKg777xTL730Ur33derUKb366qtKT0/XuXPnFBkZqZEjR2rmzJkKCwvzwugBAAAAz/NpQP/rX/+qgwcPKiQkRFFRUTpy5EiD9pObm6uJEyeqsLBQI0eOVGJiovbu3auUlBSlp6dryZIlCg8P9/DoAQAAAM/zaUB/4oknFBUVpfj4eG3btk1Tp05t0H6efvppFRYWas6cOZoyZUrl8nnz5unvf/+7XnnlFT3zzDOeGjYAAADgNT6tQR8yZIgSEhJksVgavI/c3FxlZGTIarVq0qRJDutmzZqlkJAQffzxxyouLr7a4QIAAABe1+QvEs3MzJQkDR8+XH5+jocTGhqq/v37q6SkRHv27PHF8AAAAIB6afIB3V63npCQ4HJ9fHy8JOno0aONNSQAAACgwXxag+4JRUVFkqS2bdu6XG9ffuHChTrtr7S0VFlZWZ4ZXD1dunTJZ6+NxsE5bhk4zy0D57n54xy3DGY8z00+oHtaUFCQkpOTffLaWVlZPnttNA7OccvAeW4ZOM/NH+e4ZfDleXb3h0GTD+ihoaGS3M+Q25e7m2EHAABAy3Kl4orKKspUdqVMZRVlvh6OkyYf0BMTEyVJx44dc7k+JydHktS1a9fGGhIAAECzU2GrqAy07r5evnK51m0a/NWD+7fJVnlcPcN7an+v/T78yTpr8gF98ODBkqSMjAxVVFQ4dHIpKirSrl27FBwcrD59+vhqiAAAoIWy2Wz1Co9eCbge2n+FraJRfmatLK0U0CpAAX4BNX4NbBVY+d+t/Vu737aW/YSXmu9mlk0moJeVlSk3N1cBAQGKi4urXB4XF6fhw4crIyNDixcvdrhR0YIFC1RcXKwJEyYoJCTEF8MGAAD1ZLPZVF5R3qAQ6ZGA+3//XXiuUMF7gq9q/1dsVxrlZ+Zn8as1iFb/2iawTYMCbV2/Vg3Qdf3q7+cvP0vjNhk02wWiko8DempqqlJTUyVJp0+fliR98cUXmj17tiQpPDxcjz/+uCSpoKBAo0ePltVq1fr16x3289RTT2nixImaO3eutm7dqqSkJO3Zs0eZmZlKSEjQww8/3IhHBQBA47PZbLpiu3LV4fSqAq6H9l9eUd4oPzOLLDWGxYryCoWWhTosb+3fWm0D27oOoB4Ktg0JuI0dauFdPg3oWVlZ+vDDDx2WHT9+XMePH5ckWa3WyoBek7i4OK1YsULz589Xenq6Nm/erIiICE2dOlUzZ85UWFiYV8YPAGj6ql8sVtev2Sezddjv8NUFXA8H6MbSkMAZEhDiPoB6ONjWNeC28mtV43HSxQW+4tOAPmvWLM2aNatO28bExOjQoUNu13fp0kXz5s3z1NAAADWoy8ViXp3B9eD+q14s5k3+fv71Dpi11dUGtgr0SrCtKeC2srSSxWJplJ8Z0FI1mRp0AGjq7HW1l69c9kqwrVPA9dD+zXaxWNWv9amrvZqAm388X9ckXlOngOvv50+oBVBnBHQApuaqrrZRA24Dnl9cWix9IlPU1dY1gAb7B6tdULt6XzDm7RlcM9fVZhVnKTma8gcAnkdAB5qpmupqGyXgenB/jaW+ATTIP0ihfqFOyy9euKiIDhF1CqHeCLi11dUCAMyNgA5UUde62oYG0PxT+QovCK+1/MATAdcsdbWuAqiri8UaeqGYpwKuJ+tqubAMAHA1COi4avW9CYNXZnA9tH8z1tXaA+jV3ITBWwGXuloAADyPgG4SRZeLdLrktNqcb+OZC8HqGWivJuCa+SYMIQEhrgOoB8sJ6hNwv/7qa13f63pT19UCAADfIqCbwIkLJ9RtfjeVlJd4fN+13YTBbV1tYKjrAOqFC8HqGnCbQ6gNbBXYLI4DAAB4DwHdBCLbRGrRfyzSwWMHFWeN82jA5WIxAACApoWAbgL+fv667/r7lOXPhWUAAAAtHZ+1AwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJ+Pt6AJJ06tQpvfrqq0pPT9e5c+cUGRmpkSNHaubMmQoLC6vzfnbs2KFFixbp0KFDOn36tDp27KhrrrlGU6ZM0c033+zFIwAAAAA8w+cBPTc3VxMnTlRhYaFGjhypxMRE7d27VykpKUpPT9eSJUsUHh5e637ef/99Pf300woJCdGoUaMUFRWlU6dOad26ddq8ebMeeugh/eIXv2iEIwIAAAAazucB/emnn1ZhYaHmzJmjKVOmVC6fN2+e/v73v+uVV17RM888U+M+ysrK9Kc//UlBQUFasWKFEhMTK9dlZ2frrrvu0t/+9jfdf//9CgwM9NqxAAAAAFfLpzXoubm5ysjIkNVq1aRJkxzWzZo1SyEhIfr4449VXFxc437Onz+vCxcuKCEhwSGcS1JSUpISEhJ06dIlXbx40ePHAAAAAHiSTwN6ZmamJGn48OHy83McSmhoqPr376+SkhLt2bOnxv107NhRHTp00LFjx3Ts2DGHdUePHlVOTo6Sk5PrVCoDAAAA+JJPS1yOHDkiSUpISHC5Pj4+XhkZGTp69KiGDh3qdj8Wi0X/8z//o8cee0xjx47V7bffrsjISBUUFGjdunXq1q2bXnnlFW8cAgAAAOBRPg3oRUVFkqS2bdu6XG9ffuHChVr39eMf/1iRkZF65JFHtGrVqsrlnTp10rhx4xQbG1unMZWWliorK6tO23rapUuXfPbaaByc45aB89wycJ6bP85xy2DG8+zzi0Q95aOPPtLvfvc73X777XrwwQdltVqVn5+vv/zlL3rmmWe0bds2vfrqq7XuJygoSMnJyY0wYmdZWVk+e200Ds5xy8B5bhk4z80f57hl8OV5dveHgU9r0ENDQyW5nyG3L3c3w2539OhR/fa3v1W3bt304osvKikpSa1bt1ZSUpJefPFF9erVS2vXrq2seQcAAADMyqcB3d5xpfqFnXY5OTmSpK5du9a4n88++0xlZWUaNGiQ08Wmfn5+GjhwoCRp//79VzliAAAAwLt8GtAHDx4sScrIyFBFRYXDuqKiIu3atUvBwcHq06dPjfu5fPmyJOnMmTMu19uXBwQEXO2QAQAAAK/yaUCPi4vT8OHDlZ+fr8WLFzusW7BggYqLi/Uf//EfCgkJqVyenZ2t7Oxsh20HDBggSfr3v/+tgwcPOqzLysrSv//9b1ksFg0ZMsRLRwIAAAB4hs8vEn3qqac0ceJEzZ07V1u3blVSUpL27NmjzMxMJSQk6OGHH3bYfvTo0ZKkQ4cOVS7r3bu3xo4dq5UrV2r8+PG6/fbbFR0drfz8fKWmpqqsrEzTpk3TNddc06jHBgAAANSXzwN6XFycVqxYofnz5ys9PV2bN29WRESEpk6dqpkzZyosLKxO+3nuuec0cOBAffjhh8rIyNDFixcVGhqqG264Qffee69+8pOfePlIAAAAgKvn84AuSV26dNG8efPqtG3VmfOqLBaLxo4dq7Fjx3pyaAAAAECj8mkNOgAAAABHBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAFxISEjw9RDQCMx4ngnoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAAA0SI8ePTRlyhSv7X/lypXq0aOHVq5c6bXXMCN/Xw8AAAAADdOjR496bT9v3jyNHTvWS6OBpxDQAQAAmqiZM2c6LXvnnXd04cIFTZ06Ve3atXNYl5yc7NHX/+STTxQcHOzRfYKADgAA0GTNmjXLadmHH36oCxcuaNq0aYqJifHq6yclJXl1/y2VKQL6qVOn9Oqrryo9PV3nzp1TZGSkRo4cqZkzZyosLKxe+9q/f7/eeustbd++XWfOnFG7du2UmJio8ePH66677vLOAQAAAJjclClTtG3bNu3bt08LFy7UP//5T+Xn5+uOO+7QH//4R124cEEffPCBNm/erGPHjunMmTMKDQ1V37599cADD6hfv35O++zRo4cGDRqkd999t3LZggUL9NprryklJUVnz57Vm2++qcOHDysoKEg33nijZs+erc6dO1/18Xz55Zd64403tGPHDl24cEERERG65ZZb9OCDDyoyMtJh22+//VaLFi3S+vXrVVBQIH9/f3Xs2FH9+vXT/fffr2uuuUaSZLPZtGrVKn3wwQc6duyYLl68qA4dOqhbt24aN26cRo8efdXjrgufB/Tc3FxNnDhRhYWFGjlypBITE7V3716lpKQoPT1dS5YsUXh4eJ329d577+nZZ59Vu3btdOutt6pz5846d+6cDh8+rE2bNhHQAQBAi/fLX/5S+/bt080336xRo0apY8eOkqTs7Gz9+c9/1oABA3TrrbeqXbt2OnnypNavX6/09HT99a9/1c0331zn13n//fe1fv16jRgxQgMHDtTevXv1ySef6ODBg/roo48UGBjY4GPYsGFD5acHP/zhDxUdHa39+/dryZIlSktL0/vvv6/Y2FhJUklJie677z7l5ubqxhtv1IgRI2Sz2XTixAmlpaXptttuqwzor7zyit544w3FxMToxz/+sdq2bavTp09r3759Wrt2bcsJ6E8//bQKCws1Z84ch6uA582bp7///e965ZVX9Mwzz9S6n4yMDM2dO1c33nijXn31VYWGhjqsLysr8/jYAQAAmpr8/Hz985//VIcOHRyWJyUlafPmzU7LT506pfHjx2vevHn1Cujp6elavny5w4WsjzzyiFavXq3U1NQGh92LFy9q9uzZunLlit59910NGDCgct3ChQv18ssv66mnntJbb70lSdq6datyc3M1bdo0Pfnkkw77unz5sr777rvK7z/44AN17txZq1evdqqtP3PmTIPG2xA+Dei5ubnKyMiQ1WrVpEmTHNbNmjVLS5cu1ccff6zZs2crJCSkxn298MILat26tV566SWncC5JAQEBHh07AAAwt5QU6f8yWoNUVATKz8sNqWfMkKZO9e5rVPerX/3KKYRLUtu2bV1uHxUVpR/96Ed69913deLECUVHR9fpdaZMmeLUZeaee+7R6tWrtW/fvgYH9LS0NJ07d0533HGHQziXpBkzZugf//iHPvvsM6extm7d2mlfgYGBatOmjcMyf39/tWrVymlbVz8zb/FpQM/MzJQkDR8+XH7V/gWEhoaqf//+ysjI0J49ezR06FC3+/nqq6906NAhjRo1Su3bt9fnn3+u/fv3y2KxKDk5WYMHD3baPwAAQEvUu3dvt+t27typlJQUffHFFyosLHSqQCgoKKhzQL/++uudlnXp0kWSdP78+XqM2NGBAwckSUOGDHFa5+/vr4EDByo/P18HDhxQdHS0Bg0apM6dO2vhwoXav3+/brnlFvXv31/JyclOQfzOO+/Uu+++q9GjR+vHP/6xBg4cqH79+rn948VbfBrQjxw5IklKSEhwuT4+Pl4ZGRk6evRojQF93759kqSOHTtqypQp2r59u8P67t2767XXXlN8fLxnBg4AAExv6tSrm50uKbncLFsIRkREuFy+bt06/fKXv1RQUJCGDRumuLg4BQcHy8/PT9u2bdO2bdt0+fLlOr+Oq1BrD8QVFRUNG7ykCxcuSHJ/HPbl9u1CQ0O1dOlSzZ8/X+vXr1dGRoYkKTw8XD/96U81ffr0yvP8xBNPKCYmRitXrtTChQu1cOFC+fv76+abb9bs2bMbLUv6NKAXFRVJcv+Rin25/QfsTmFhoSRp+fLllX8h3XDDDfr222/1+uuv6+OPP9Z//dd/6Z///GetFySUlpYqKyurvofiEZcuXfLZa6NxcI5bBs5zy8B5Ngd3k3yeYLPZVFJS4rX9e4s9/F66dMlh/FeuXKlc7sorr7yigIAALV68WImJiQ7rTp48qW3btqm0tNTpZ3LlyhWHZfZZd1fb2l+7vLy8Tj9b+x8Ely9frtzeHqZPnDjhch8nT56UZJSv2NeHhYXpd7/7nebMmaPs7Gxt375dH3zwgV5//XWVlpY69JOfMGGCJkyYoDNnzmj37t1au3at1q1bp8OHD2vFihV1urj12LFjtW5TE59fJOoJNptNkvE/yJ/+9KfKNkChoaF64YUXdOTIEX355Zf69NNPdccdd9S4r6CgII838a+rrKwsn702GgfnuGXgPLcMnOfmr6SkpEnOoNvLelu3bu0wfvvstbtjOn78uK655hr16tXLYXlFRYX27NkjychJ1Z/fqlUrh2X26/5cbWuvA/f396/Tz9YehgMDAyu3t5fO7N69Wz/96U8dti8vL9cXX3whSerXr5/L17j++ut1/fXX68c//rFuvfVWbdy4UY899pjTdlarVVarVXfccYemTZumzz//XMePH9d1111X67jr+rvB3R/5Pi3Mtl/M6W6G3L68trof+/qIiAinHp0Wi0UjR46UJO3du/eqxgsAANBcWa1WHTt2TAUFBZXLbDabFixYoK+//tqHI3Nkv+bwX//6V2UYt3vnnXeUl5enYcOGVdbKHz58WN9++63TfuzL7H80XL58WTt37nTarqysrLJmvrH+YPPpDLr94xN3HwPk5ORIkrp27Vrjfuzr3QV5+82O3H2kAwAA0NJNnz5dTz31lO6++2794Ac/kL+/v3bt2qXs7Gzddttt2rBhg6+HKElq06aNnn32WT300EOaPHmyfvSjH1X2Qc/IyFBERIRDi+7PPvtML774ovr27auEhAR17NhRp06dUlpamvz8/DRt2jRJRk786U9/qvj4ePXq1UvR0dEqLS3Vli1blJ2drREjRjTanVN9GtAHDx4syehhXlFR4dBppaioSLt27VJwcLD69OlT43769u2rkJAQ5efnq7i42Kkl41dffSVJXr/dLQAAQFM1ceJEBQYG6p133tGqVasUFBSkAQMGaN68efr0009NE9AlYxb9/fff1xtvvKGMjAwVFRWpU6dOmjhxoh588EGHO5XedNNNOnnypLZv3660tDQVFRUpMjJSN954o6ZPn15ZjhIcHKxHH31UmZmZ2r17t1JTU9WmTRvFxcXp97//vcaNG9dox2ex2Qu4feT+++9XRkaG2xsVTZgwweGvoOzsbEly+gtm7ty5evfddzVt2jQ98cQTslgskqRDhw7pnnvu0ZUrV7RmzRrFxcXVOB5f1hRSz9j8cY5bBs5zy8B5bv6aag066seX59nd7xGfXyT61FNPaeLEiZo7d662bt2qpKQk7dmzR5mZmUpISNDDDz/ssL29qf2hQ4cclj/00EPasWOH3nnnHX3xxRfq37+/vv32W61bt06lpaV68sknaw3nAAAAgK/5PKDHxcVpxYoVmj9/vtLT07V582ZFRERo6tSpmjlzZmX9eG1CQ0O1ePFiLVy4UGvXrtV7772n1q1b64YbbtCMGTM0fPhwLx8JAAAAcPV8HtAl465S8+bNq9O21WfOq2rTpo0efvhhp1l3AAAAoKnwaZtFAAAAAI4I6AAAAICJENABAAAAE/FIDXp5ebnS0tJ0/vx53XbbbYqIiPDEbgEAAIAWp94B/YUXXlBmZqZWrFghybgF7M9+9jPt2LFDNptN7du319KlS2lpCAAAADRAvUtc0tPTNWDAgMrv169fr+3bt+v+++/Xyy+/LElauHCh50YIAAAAtCD1nkE/deqU4uPjK7/fsGGDYmJi9Oijj0qSDh8+rH/+85+eGyEAAADQgtR7Br2srEz+/t/n+szMTA0bNqzy+9jYWJ0+fdozowMAAABamHoH9KioKO3evVuSMVt+/PhxDRw4sHJ9YWGhQkJCPDdCAAAAoAWpd4nLT37yE/3lL3/RmTNndPjwYYWGhuqWW26pXJ+VlcUFogAAAEAD1XsG/YEHHtDdd9+tL774QhaLRc8//7zatWsnSbpw4YLWr1+voUOHenygAAAA8I3Zs2erR48eysvLq9P2U6ZMUY8ePbw8quar3gE9MDBQzz33nDIzM5WWlqaRI0dWrmvTpo0yMjI0c+ZMjw4SAAAAzh555BH16NFDixcvrnXbGTNmqEePHlq3bl0jjAxXw6N3Ei0vL1fbtm0VEBDgyd0CAADAhXvvvVeStHz58hq3y8vL05YtWxQREaHbbrutMYaGq1DvgL5p0yYtWLDAYdnixYvVv39/9e3bV4888ojKyso8NkAAAAC4NnjwYCUkJOjAgQPav3+/2+2WL18um82mcePGOXTjgznV+wwtWrRIHTt2rPw+Oztbzz33nGJjYxUTE6NPPvlE119/vaZPn+7JcQIAAMCFe++9Vy+88IKWLl2qp59+2mn9lStXtHLlSlksFo0fP16SlJqaqrVr12rfvn0qKCiQJCUmJuquu+7S5MmT5efn0SKLShUVFfrggw+0fPlyHTlyRDabTUlJSRo3bpwmTpzo9Lo7duzQm2++qQMHDujMmTMKCwuT1WrVzTff7FBS/e2332rRokVav369CgoK5O/vr44dO6pfv376f//v/yk2NtYrx+Mt9f7pHzlyRNddd13l95988omCgoK0fPlyvfnmmxo9erRWrVrlyTECAADAjbvvvlsBAQH617/+pZKSEqf1mzdvVkFBgYYNG1YZVF966SUdOHBAvXv31uTJkzVmzBhdvHhRzz77rB5//HGvjfWxxx7T73//exUWFmr8+PG69957dfbsWT399NN67LHHnMY9ZcoU7dy5U0OHDtWMGTM0cuRIBQYG6v3336/crqSkRPfdd5/eeustWa1W3XfffRo/frx69OihtLQ0ff311147Hm+p9wz6+fPnFR4eXvn9li1bNGTIEIWGhkqSBg0apE2bNnluhAAAAHCrQ4cOGjVqlNasWaM1a9Zo7NixDuuXLl0q6ft6dUlauHChU1vsiooKPfHEE1q1apUmT56sPn36eHScq1ev1urVq9WzZ0+99957atOmjSTpoYce0uTJk7V69WrdeuutuvPOOyVJy5YtU0VFhd59911de+21Dvs6c+ZM5X9v3bpVubm5mjZtmp588kmH7S5fvqzLly979DgaQ70Denh4uE6cOCFJKioq0r59+/TrX/+6cn15ebmuXLniuRECAAA0REqK9NZbDX56YEWF5KVSj0ozZkhTp171biZMmKA1a9Zo2bJlDgH9m2++0ebNm9WxY0eHznuu7lnj5+enqVOnatWqVUpPT/d4QF+xYoUko/OMPZxLUkhIiB577DFNnz5dy5YtqwzodkFBQU776tChg9Oy1q1bOy0LDAxUYGDg1Q690dU7oPft21f/+Mc/1K1bN23evFlXrlzRzTffXLk+JydHkZGRHh0kAAAA3BsyZIji4uK0a9cuZWdnKykpSZK0cuVKlZeXV5bB2J09e1aLFi3Spk2blJeXp+LiYof9ffPNNx4f44EDB+Tn56dBgwY5rRs4cKBatWqlrKysymV33nmnPv30U91777368Y9/rCFDhqh///6KiopyeO6gQYPUuXNnLVy4UPv379ctt9yi/v37Kzk5Wa1atfL4cTSGegf0X/7yl5o6daoeeughSUbdU7du3SRJNptNqampGjx4sEcHCQAAUG9Tp17V7PTlkhIFBwd7cEDeY7FYdM899+jll1/WsmXLNHv2bNlsNi1fvlwWi8WhvOW7777T+PHjlZeXp969e2vMmDEKCwuTv7+/vvvuO6WkpHilLOTChQsKCwtzOaPt7++v8PBwFRYWVi77wQ9+oDfeeENvvfWWVq5cqQ8++ECS1KtXLz3yyCO68cYbJUmhoaFaunSp5s+fr/Xr1ysjI0OSUfXx05/+VL/4xS+aXAvwegf0bt266ZNPPtGuXbvUtm1bDRw4sHLdd999p2nTphHQAQAAGtnYsWM1f/58rVq1Sr/+9a+1c+dOHT9+XEOGDFF8fHzldsuWLVNeXp5mzpypWbNmOexj9+7dSklJ8cr42rZtq/Pnz6usrMwpMJeXl+vs2bOV1zTa3Xrrrbr11ltVXFysPXv2aOPGjVqyZIkeeOABrVq1qnKSOCoqSs8995xsNpu+/vprff7551q8eLFef/11VVRUVE4sNxUNKqxq3769RowY4RDOJSksLEzTpk1zKuQHAACAd3Xq1EkjRozQ2bNnlZqaqmXLlkky6tOrysnJkWTMUFe3fft2r40vOTlZFRUV2rFjh8vXvXLlinr27OnyuSEhIRo6dKieeOIJPfDAAyorK9PmzZudtrNYLLrmmms0ZcoUvf3225KktLQ0zx5II2jwlQ+5ubl6++239cwzz+iZZ57R22+/rdzcXE+ODQAAAPVwzz33SJLefvttrVu3TuHh4Ro1apTDNjExMZKkbdu2OSw/cOCA3njjDa+Nbdy4cZKkl19+2aEdZElJiV5++WVJquzTLhmhvby83Gk/9jIY+0Whhw8f1rfffuu0nX2Zq4tHza5Bt5L685//rP/93/916tby4osv6oEHHtCvfvUrjwwOAAAAdTd8+HBZrVbt3btXkjR58mSnmu8xY8Zo0aJFeu6555SZman4+Hjl5ORo48aNuv322/XJJ594ZWx33nmn0tLStGbNGv3kJz/RqFGjZLFYlJqaqry8PI0ePVr/8R//Ubn93LlzVVBQoP79+8tqtSogIED79+/X559/LqvVqp/85CeSpM8++0wvvvii+vbtq4SEBHXs2FGnTp1SWlqa/Pz8dP/993vleLyp3gF9+fLl+tvf/qZ+/frp5z//ua655hpJxl8vixYt0t/+9jfFxsY69eAEAACAd9kvFv3zn/8s6fsZ9ao6d+6sxYsX66WXXtLOnTuVkZGhxMREPfXUUxo6dKjXArok/elPf9LAgQO1YsWKyos+k5KSNGPGDN13330O2z7wwANKTU3Vl19+qa1bt8pisSg6Olr//d//rWnTpiksLEySdNNNN+nkyZPavn270tLSVFRUpMjISN14442aPn26+vfv77Xj8RaLzWaz1ecJY8eOVUBAgBYvXix/f8d8X15erkmTJqmsrEwrV6706EAbS1ZWlpKTk1vca6NxcI5bBs5zy8B5bv5KmlAXFzScL8+zu98j9a5Bz87O1ujRo53CuWS0yBk9erSys7MbNkoAAACghat3QA8ICHBqZl/VxYsXm1yvSQAAAMAs6h3Qr7/+en3wwQcur5YtLCzU0qVLPX5rWAAAAKClqPdFog8++KCmT5+u0aNHa9y4cZUN4r/++mutXLlSFy9e1EsvveTxgQIAAAAtQb0D+sCBA7VgwQL94Q9/qGwAbxcdHa3nn39eAwYM8NgAAQAAgJakQX3QR4wYoVtvvVVffvml8vLyJEmxsbHq1auXli5dqtGjR3u1RQ8AAADQXDUooEuSn5+fevfurd69ezssP3v2rI4ePXrVAwMAAABaonpfJAoAAADAewjoAAAAgIkQ0AEAAAATIaADAAAAJlKni0Srt1Osya5duxo8GAAAAKClq1NAf/755+u1U4vF0qDBAAAAAC1dnQJ6SkqKt8cBAAAAk5o9e7Y+/PBDpaWlKSYmxtfDafbqFNAHDRrk7XEAAACgnh555BGtXr1a//M//6NJkybVuO2MGTP02Wef6bXXXtPtt9/u1XHZA707gwYN0rvvvlv5/bJly7Rv3z5lZWXpq6++0qVLl/Tf//3fevjhh706TrNq8I2KAAAA4Fv33nuvVq9ereXLl9cY0PPy8rRlyxZFRETotttua7TxjRw5UsnJyU7LrVarw/fPP/+8Lly4oLCwMEVGRio3N7exhmhKBHQAAIAmavDgwUpISNCBAwe0f/9+9erVy+V2y5cvl81m07hx4+Tv33jxb9SoURo7dmyt2/3pT39SUlKSrFarVq5cqSeeeKIRRmdetFkEAABowu69915J0tKlS12uv3LlilauXCmLxaLx48dLklJTU/Xoo4/qhz/8ofr27au+fftq7NixSklJUUVFRaON3e7mm292mlVvyQjoAAAATdjdd9+tgIAA/etf/1JJSYnT+s2bN6ugoEDDhg1TbGysJOmll17SgQMH1Lt3b02ePFljxozRxYsX9eyzz+rxxx9v7ENANZS4AAAANGEdOnTQqFGjtGbNGq1Zs8appMQ+s26faZekhQsXKi4uzmG7iooKPfHEE1q1apUmT56sPn36XPXYUlNTlZ+f77R82rRpateu3VXvv7kioAMAgGYpZU+K3tr9VoOfX1FRIT8/7xYbzOg3Q1P7TL3q/UyYMEFr1qzRsmXLHAL6N998o82bN6tjx44aOXJk5fLq4VyS/Pz8NHXqVK1atUrp6ekeCehpaWlKS0tzWn733XcT0GtAQAcAAGjihgwZori4OO3atUvZ2dlKSkqSJK1cuVLl5eWVZTB2Z8+e1aJFi7Rp0ybl5eWpuLjYYX/ffPONR8Y1b968Ol0kCkcEdAAA0CxN7TP1qmanS0pKFBwc7MEReY/FYtE999yjl19+WcuWLdPs2bNls9m0fPlyWSwWh/KW7777TuPHj1deXp569+6tMWPGKCwsTP7+/vruu++UkpKiy5cv+/BoQEAHAABoBsaOHav58+dr1apV+vWvf62dO3fq+PHjGjJkiOLj4yu3W7ZsmfLy8jRz5kzNmjXLYR+7d+/mDvImQBcXAACAZqBTp04aMWKEzp49q9TUVC1btkySUZ9eVU5OjiTpBz/4gdM+tm/f7v2BolYEdAAAgGbinnvukSS9/fbbWrduncLDwzVq1CiHbWJiYiRJ27Ztc1h+4MABvfHGG40zUNSIEhcAAIBmYvjw4bJardq7d68kafLkyQoMDHTYZsyYMVq0aJGee+45ZWZmKj4+Xjk5Odq4caNuv/12ffLJJ40+7mXLlmnnzp2Svp/h37BhgwoKCiRJiYmJ+q//+q9GH5evENABAACaCfvFon/+858lfT+jXlXnzp21ePFivfTSS9q5c6cyMjKUmJiop556SkOHDvVJQN+5c6c+/PBDh2WHDh3SoUOHJEmDBg1qUQHdYrPZbL4ehJlkZWUpOTm5xb02GgfnuGXgPLcMnOfmryl1cUHD+fI8u/s9Qg06AAAAYCIEdAAAAMBECOgAAACAiRDQAQAAABMhoAMAAAAmQkAHAAAATISADgAAAJgIAR0AAAAwEQI6AAAAYCIEdAAAAMBECOgAAACAiRDQAQAAXDh27Jivh4BGYMbzTEAHAAAATISADgAAAJgIAR0AAAAwEQI6AAAAYCIEdAAAAMBETBHQT506pSeeeELDhw/XddddpxEjRujZZ5/V+fPnG7zP7du3Kzk5WT169NArr7ziwdECAAAA3uPv6wHk5uZq4sSJKiws1MiRI5WYmKi9e/cqJSVF6enpWrJkicLDw+u1z6KiIj3++ONq3bq1iouLvTRyAAAAwPN8PoP+9NNPq7CwUHPmzNFf/vIXPfroo0pJSdH06dN19OjRBs1+P/vssyoqKtIDDzzghREDAAAA3uPTgJ6bm6uMjAxZrVZNmjTJYd2sWbMUEhKijz/+uF6z4KmpqVq5cqV++9vfKjIy0tNDBgAAALzKpwE9MzNTkjR8+HD5+TkOJTQ0VP3791dJSYn27NlTp/0VFhbqd7/7nUaNGqUxY8Z4fLwAAACAt/k0oB85ckSSlJCQ4HJ9fHy8JOno0aN12t+cOXNUUVGhp59+2iPjAwAAABqbTy8SLSoqkiS1bdvW5Xr78gsXLtS6r+XLl2v9+vV65ZVX1KlTpwaPqbS0VFlZWQ1+/tW4dOmSz14bjYNz3DJwnlsGznPzxzluGcx4nn3excUT8vLy9Nxzz+lHP/qRRo8efVX7CgoKUnJysodGVj9ZWVk+e200Ds5xy8B5bhk4z80f57hl8OV5dveHgU9LXEJDQyW5nyG3L3c3w2735JNPqnXr1nrqqac8O0AAAACgkfl0Bj0xMVGSdOzYMZfrc3JyJEldu3atcT8HDhzQhQsXNHToUJfr//a3v+lvf/ubRo4cqb/85S8NHzAAAADgZT4N6IMHD5YkZWRkqKKiwqGTS1FRkXbt2qXg4GD16dOnxv3cddddKikpcVqek5NTeUfRXr16qWfPnp49AAAAAMDDfBrQ4+LiNHz4cGVkZGjx4sWaMmVK5boFCxaouLhYEyZMUEhISOXy7OxsSVJSUlLlsjlz5rjc/8qVK7V9+3bdcsstevjhh710FAAAAIDn+Pwi0aeeekoTJ07U3LlztXXrViUlJWnPnj3KzMxUQkKCU7C2XwR66NAhXwwXAAAA8CqfXiQqGbPoK1as0NixY7V37169/fbbOn78uKZOnaqlS5cqPDzc10MEAAAAGo3PZ9AlqUuXLpo3b16dtq3PzPnYsWM1duzYhg4LAAAAaHQ+n0EHAAAA8D0COgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHQAAADARAjoAAAAgIkQ0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAiBHSTKCuTbDZfjwIAAAC+5u/rAUDKz5e6d5dsth5KTJTi441HQoLjf3fuLFksvh4tAAAAvImAbgJRUdL8+dKmTed04UIH5eRIW7dKZ886bhcUJMXFOQd3+9foaKlVKx8cAAAAADyGgG4CrVpJ998vDRtWoOTkDpXLL1yQcnKkY8eMr1X/+5//lAoKHPfj7y/FxLgO8PHxUmysFBjYiAcGAACAeiOgm1jbttJ11xkPV0pKpNxc1yE+Lc0onala126xGLPsrspn7P8dHOz94wIAAIB7BPQmLDhY6tHDeLhy+bKUl+c6wG/dKi1dKpWXOz4nMtJ9DXx8vNSunXePCQAAoKUjoDdjgYFSYqLxcOXKFenECefymZwcae9eafVq6dIlx+e0b+9+9j0hQerQgQtZAQAArgYBvQVr1cqoS4+NlYYPd15vs0nffOO6Bj472yijKSpyfE6bNjWX0HTuLPnR3BMAAMAtAjrcsliMQN25szR4sPN6m83oNOPuQtbPP5fOnHF8jr0TjbsQb7XSiQYAALRsBHQ0mMVilLR06CD16+d6G3snGlch3lUnGvusvrsATycaAADQ3BHQ4VX16URTPcTX1onGVYiPi5NCQhrhwAAAALyEgA6fqq0TTVmZ0YnGHtyrBvjPP5eWLXPuRBMRUfOFrHSiAQAAZkZAh6kFBEhduxoPV65ckU6edF0Dv2+f+040NV3I2rEjnWgAAIDvENDRpLVqZdw9NSam5k40rmrga+tE4y7E04kGAAB4EwEdzVrVTjSDBjmvr9qJxlWIz8ysfyea6GjJn39ZAACggYgRaNHq24mmeoivrRNN9RBfXh6gpCQ60QAAAPcI6EAtrqYTzfr1xt1aKyrsW3eTxSJ16eK+Bj4+nk40AAC0ZAR04CrVpxPN1q0nVFYWXRniMzPdd6Kp6ULWsDDvHhMAAPAdAjrgZVU70URFnVdycrTDensnGlc18LV1oqka3OlEAwBA80BAB3ysaieaG290Xl+1E031EH/kiLRhg1EnXxWdaAAAaLoI6IDJ1aUTzblzrnvBu+tEExjoWPNePcTTiQYAAN/hLRho4iwWKTzceDSkE83q1a470cTEuK+Bj4012k0CAADPI6ADLUBtnWguXTI60biahd+wQcrPr9qJRpWdaGq6kJVONAAANAwBHYBat5a6dzcertg70bi7mdPy5cY2Vdk70bgL8XSiAQDANQI6gFpV7UTjStVONNVD/JdfSv/6l3MnmrAw97PvCQl0ogEAtFwEdABXrS6daE6fdl1Cc/So6040ISE1B3g60QAAmisCOgCvs1ikyEjjUVMnGlclNDk50rZtUmGh43MCA6W4OPch3mqlEw0AoGni7QuAz1XtRNO3r+ttiorcB/h//Us6dcpxe/usvrsaeDrRAADMioAOoEkIDZV69TIertg70bgK8bV1onEX4ulEAwDwBQI6gGahPp1oqof4bdukFSucO9F06uQ6uNOJBgDgTQR0AC1CXTrRnDrluoRm//76daKpqGitiAg60QAAGoaADgAyatatVuNRUycaVyU0zp1ojL8CQkJqvplTVBSdaAAAzgjoAFAHVTvRDBzovL5qJ5r09OOqqIitcycadyGeTjQA0DLxqx8APKBqJ5qgoCIlJztvY+9E42oWvrZONK5CPJ1oAKB5IqADQCOpTyea6iF+40bXnWiiotyX0MTHS23aeP+4AACeRUAHAJOoSyea/HzXF7Ju3+6+E01NdfDt23v1kAAADUBAB4AmIiDACNcJCa7X2zvRuCqh2b9f+uQTqaTE8TlhYTUH+E6d6EQDAI2NgA4AzUTVTjTDhjmvr9qJpnqIP3rUKKP57jvH59g70bgL8XSiAQDPI6CbxaFDCsjJMd71goN9PRoAzVBtnWgkoxONPbhXn4Xfvr3mTjSuQjydaACg/vi1aQa5udK116qb/fvw8O+nwaxWo41D1e+tVj53BuAV7dtLffsaD1eqdqKpHuI/+cR1Jxqr1X0JTVwcnWgAoDoCuhnExUmZmTqRlqZom824Csz+2LNHKigwPpuuKjBQio52Hd7tj+ho3vkAeFRdOtEcP+76QtZNm6S8PMdONJLUpUvNdfB0ogHQ0hDQzWLQIJ1v21bRrponl5UZ01JVg3t+vvFOl58v7dwpffyx89VfkhQR4T7A28N9+/bMxgPwiNatpWuuMR6u2DvRuCuhoRMNABDQm4aAAOOOJLGx7rex38bQVYC3P7ZtM64Qqy442H14tz+iooxxAMBVqNqJ5pZbnNdXVEgnT7ouoamtE427EE9FIICmhoDeXFS9jeF117nfrrTUePerHt7tj61bja+XLzvvv3PnmuvirVapXTvvHieAZs3Pr/ZONN9+6/5C1k2bau5E4yrE04kGgNkQ0FuaoKCaGylL378Dugrw+fnSkSNSerp09qzzc0NDa66Lt1qNoN+qlbeOEEAzZrEYlXsREXXrRFM9xLvrRBMb676EJiaGTjQAGpcpfuWcOnVKr776qtLT03Xu3DlFRkZq5MiRmjlzpsLCwmp9fnFxsVJTU7Vp0ybt379fp06dksViUdeuXXXHHXdo8uTJCgwMbIQjaSaqvgO6a+UgScXF0okT7mvjN2wwZuvLyx2f16qVcVVYbbXxISFePUwAzVNdOtHk5roO8WvWGL+2qrJ3oqke3C2WEPn704kGgOf5PKDn5uZq4sSJKiws1MiRI5WYmKi9e/cqJSVF6enpWrJkicLDw2vcx44dO/TYY4+pffv2Gjx4sEaNGqXvvvtO69ev1/PPP69PP/1U77zzjoL4DepZISFSt27Gw52KCumbb9zXxWdlSampzp9JS8a7bG218Z068dk0gHoJDZV69jQertg70bgrofm+E0185XPoRAPAk3we0J9++mkVFhZqzpw5mjJlSuXyefPm6e9//7teeeUVPfPMMzXuIyIiQi+++KJ+9KMfOcyU/+Y3v9HUqVO1e/duLV68WDNmzPDaccANPz+jwDMqSrrhBvfbFRW5DvD2x5dfGp1sqvdnCwgw2knWVBcfHW20lgCAOqhrJ5rNm3MkxTuV0NTUicZdiKcTDYCqfBrQc3NzlZGRIavVqkmTJjmsmzVrlpYuXaqPP/5Ys2fPVkgN5Q7JyclKdtGeMDQ0VD/72c/06KOPatu2bQR0MwsNlXr0MB7ulJe7bjdpf+zeLa1ebZTeVNexY+218R060OoBQK3snWhKSorlqjNuTZ1oDhwwymiqd6Jp18797HtCAp1ogJbGpwE9MzNTkjR8+HD5VStTCA0NVf/+/ZWRkaE9e/Zo6NChDXoN//+7sqcVFyU2ff7+RsiOiXG/jc0mnT/vvi4+P1/ascMou6mudeva6+K7dKHdJIAa1bcTTfUQ76oTTXBwzSU0XbpQ7Qc0Jz4N6EeOHJEkJbjpKBIfH6+MjAwdPXq0wQF9xYoVkqSbbrqpQc9HE2OxGJ8Vt2/v/laHktFG8uRJ97XxmZnG19JS5/1HRtZcF29vN8l0FwAX6tqJxlUNfE6OMcfw7beO29s70bgL8XSiAZoWn/5zLSoqkiS1bdvW5Xr78gsXLjRo/++9957S09OVnJyscePGNWyQaJ4CA79/B3PHZpPOnHFfF5+TI332mbFNdW3auAzwbW026cKF72/+xCc7AFywzzP06eN6/cWLrmffjx1z3YnGz8/4VeSuhIZONIC5NNu/pz/99FM999xzioiI0IIFCxRQx7KE0tJSZWVleXl0rl26dMlnr40aBAZKXbsaDxcsly7J/5tv5P/NNwooKJB/QYHDfwccPCj/06dlKS9X1eIcm5+fyiMiVB4ZqbLOnVUeGanyzp0d/zsyUjbaPzQ5/FtuGXx9ni0W97e1KC216ORJf504EaCTJwOUnx+oEyeM79PSAlVQ4K+KCsdP+Tp1KpfVelnR0WWyWssUHV2mLl2+/++QEFujHJeZ+Poco3GY8Tz7NKCHhoZKcj9Dbl/ubobdndTUVP36179Whw4dlJKSotjY2Do/NygoyOUFp40hKyvLZ68NL6uokE6f1tGMDHUNDJTy82XJz1dAXp4C8vMVnJ9vtH84f975uWFhNdfFW63GZ+UUoJoG/5ZbBrOf55puY1FebnwQ6Dj77q+cHH8dPmx0v63eiaZjx5ovZG2OnWjMfo7hGb48z+7+MPBpQE9MTJQkHTt2zOX6nJwcSVJXNzOXrqxZs0aPPvqoOnXqpHfeecdtfTvQqPz8pM6ddalnT7ls+2B38aL7uvj8fKMFxMmTrttNVr/5k6vaeNpNApBRj15TlV9FhdE0q2oJjf1rVpb7TjQ1XcgaEcGlOUBd+TSgDx48WJKUkZGhiooKh04uRUVF2rVrl4KDg9XHXRFeNfaWjJ07d673zDlgCm3aSN27Gw93ysulggL37Sb37TPePS9edH5uhw4194y3Wo1pMt5FgRbNz8+4hUR0tOSqR4O9E427C1lr6kTjLsTTiQb4nk8DelxcnIYPH66MjAwtXrzY4UZFCxYsUHFxsSZMmODQAz07O1uSlJSU5LCvDz/8UE8++aSio6OVkpIiq9XaOAcBNDZ//+/DtDs2m/Hu6C7E5+VJu3YZ7SZt1epKg4Jq7hcfE2O8k1a5KRiAlqVqJ5oBA1xvY+9E4yrE79zp3IkmIMC4WNVdiKcTDVoSn/+v/tRTT2nixImaO3eutm7dqqSkJO3Zs0eZmZlKSEjQww8/7LD96NGjJUmHDh2qXPb555/rySefVEVFhQYPHqyVK1c6vU7btm01ffp0rx4LYBoWi1G7Hhbm/n7mklFkWrXdZPXSmu3bpVWrjHufV1e93aSr0pqwMGbjgRaqPp1oqof4tWvr34kmNpYqPjQfPg/ocXFxWrFihebPn6/09HRt3rxZERERmjp1qmbOnKmwsLBa93HixAlV/F9Nrr3veXVWq5WADlRnn7KKi3O/jc0mnT3rvi7++HHp88+dp8MkKSSk9rr4qCimxYAWqE0bY/7A3RxCaanx68VVCU16urRkiXTliuNzoqLc18DHxxs3rQaaAlO8K3bp0kXz5s2r07ZVZ87txo4dq7Fjx3p6WAAkYwa8Qwfjcf317re7dEk6ccJ9WU1GhrG+emuI/7uAttba+Hp2cwLQtAUFSd26GQ9XXHeiMb7u3Cl9+KFxT7qqOnas+ULW9u350A/mYIqADqAZaN1aSkw0Hu5UVBgz7e7q4g8fljZuNIpXq2vXrua6eKvVKLvhKjOgRWhIJxr7f2dlGWU0xcWOz7F3orEH99atO2jQoO9DPJ1o0FgI6AAaj5+fEaIjI6V+/dxvd/Gi42x89dKatDSjQLX659v+/s7tJl3NzAcHe/c4AfhcfTrRuArx6enS+fOdHZ5TtRONq5l4OtHAUwjoAMynTRvpmmuMhztXrhhdaNzVxu/fL336qeTqRmjh4TXXxVutUqdOTJUBzVhdOtFkZh5S69Y9XIZ4d51oYmPdl9DExBjbALUhoANomlq1MqarunRx/+4q1dxuMj9f2rPH6Ctfvd1kYKAx9VZTXXx0tFEoC6BZateuQsnJNXeiyc11XQf/738bHwRW5edn/OqoGtyrBvi4ODrRwEBAB9C8tWtnPGq6g2tZmVGs6qouPj/fmCr7+GPnWydKxvRbteAe5udn/NFgD/dceQY0S23aGL9a3P16sXeicVdC464TTU0XstKJpmUgoAOA/XPpmu4+bLMZF6+66xmfny9t2yadPq3o6s8NDq69Lj4qis++gWamrp1oqs++22/mVFMnGnchnvmA5oGADgB1YbEYtevh4dJ117nfrrRUX6enq1vr1q5LarZuNb5Wf9e1WL5vN1lTbXy7dt49TgCNpj6daKrPwh88aJTRVO9E07at+9n3hAQ60TQVBHQA8KSgIJVZrTWX1NjbR7iriz9yxPj8++xZ5+eGhtZcF2+1GkG/VSvvHSOARlGXTjSFha5r4O03dDp/3vE5wcFGrbu7EN+lC78+zICADgCNrWr7iL593W9XXOx886eqZTUbNhjtJsvLHZ9nv4C2tr7xISFePUwA3mWxGA2nOnVyf638+fOua+BzcqRdu6TTpx23pxONORDQAcCsQkJqLmCVjM/A7e0mXdXFZ2VJqalGN5vq2revvTa+UycaOwNNWFiY1Lu38XDF3onGVYj/97+NOYCqTa5cdaKp+t90ovEMAjoANGV+fsYFplFR0g03uN+uqMh9z/j8fOnLL41i14oKx+cFBBifr9dUFx8dzTsy0ER5sxONuxBPJ5raEdABoCUIDZV69DAe7pSXu243aQ/2u3dLq1c7X5UmGa0laqqNj4kxLrDl6jSgSWloJxp7Cc2qVc7XxHfoUPOFrHSiIaADAOz8/Y0gHRPjfhubzShqddczPj9f2rHDKLuprnXr2uviu3ShwBVoQurSiaagwHUN/KFD7jvRuJt9bymdaAjoAIC6s1iM6a327aVevdxvd/myUbzqrjY+M9P4WlrqvP/IyJrr4u3tJpv7OzTQDPj5fX/T55o60bi7kDUjw7gFRVUtoRMNAR0A4HmBgTVPq0nGO/OZM+7r4nNypM8+M7aprk2b2nvGR0U1/XdpoJmr2onG3WU09k40rkJ8TZ1o3M3CN4VONAR0AIBvWCxG7XrHjlKfPu63KylxbjdZdWZ+0yZjffV2k/apu5p6xlutXLEGmJw3O9HEx0u9eoXVeOsKXyCgAwDMLThYSkoyHu5UVBjTaO5q4w8dktavd75ri2S8+9dUF2+1GkWvtJsETKk+nWiqh/iMDOnYsXZ64olGHXKtCOgAgKbPz8+4g2rnzlL//u63u3jRfV18fr504IAx3eaq3WS12fgOAQHGa1UN9bSbBEyntk40WVnHJZlrCp2ADgBoOdq0kbp3Nx7ulJcbbSdcldTk50v79klr1qjzxYvOz+3Qoea6eKvVKOnhAlcANSCgAwBQlb//92HaHZtNh7ZvV4/QUPdlNbt2Ge0mqxa/SsZ0Xk018TExxmx9YKB3jxOAaRHQAQCoL4tFFW3bGkWvPXu6366szLHdZPXSmu3bjTu5XLrk/Nzq7SZd1caHhTEbDzRDBHQAALwlIMBo2BwX534bm006e9Z9Xfzx49Lnn0vffuv83JCQ2nvGR0UZnwoAaDL4FwsAgC9ZLEbteocO0vXXu9/u0iX37Sbz8412FCdOGLP2VdkvoK2pLt5qNW7fCMAUCOgAADQFrVtLiYnGw52KCmOm3V3P+MOHpY0bnW/NKBl3Z62pLt5qNcpuaDcJeB0BHQCA5sLPzwjRkZFSv37ut7t40XE2vnppTVqaUTt/5Yrj8/z9Xd/8qfrsfHCwd48TaOYI6AAAtDRt2kjXXGM83LlyxehC4642fv9+6dNPpQsXnJ8bHl5zXbzVatzbnQtcAZcI6AAAwFmrVsZseZcu0oAB7rf77jv3dfH5+dKePUZf+ertJgMDpejommvjo6ONtpRAC0NABwAADdeunfFwd591ybhw9dQp9z3jd+6UPv5YKilxfm5ERO218e3bMxuPZoWADgAAvCsgQIqNNR7u2GzGxavu6uLz86Vt26TTp52fGxxce118VJQxDqAJIKADAADfs1iM2vXwcOm669xvV1pqXMDqKsDn50tbtxpfL1923n/nzrXXxrdr593jBOqAgA4AAJqOoCApIcF4uGOzuW83mZ8vHTkipacbN4iqLjS0Mrx3CQ017hRbPcR37mzU6ANeQkAHAADNi8Vi1K5HREh9+7rfrrjY+eZPVWbm2+zfL/3rX1J5uePz7BfQ1lQXb7Uad3oFGoCADgAAWqaQEKlbN+PhwtdZWUru0eP7dpOuauOzsqTUVKObTXXt29deG9+pEzd/ghMCOgAAgDt+fsYFplFR0g03uN+uqMj9xa35+dKXXxqdbCoqHJ8XEGC0k6ypLj462riTLFoMAjoAAMDVCg2VevQwHu6Ul7tuN2kP9rt3S6tXG6U31XXsWHPP+JgY4wJb2k02CwR0AACAxuDvbwTpmBj329hs0vnz7nvG5+dLO3YYZTfVtW5de8/4Ll1oN9kEENABAADMwmIxatfbt5d69XK/3eXLRrtJd7XxmZnG19JS5/1HRtZcF29vN8lsvM8Q0AEAAJqawEApPt54uGOzSWfOuK+Lz8mRPvvM2Ka6Nm1q7xkfFUW7SS8hoAMAADRHFotRu96xo9Snj/vtSkqc201WnZnftMlYX73dpJ+f+3aTVR+hod49zmaIgA4AANCSBQdLSUnGw52KCun0afd18YcOSevXG/Xz1YWF1d4zPiKCdpNVENABAABQMz8/4w6qnTtL/fu73+7iRfd18fn50oEDRu28q3aT1WfjXZXWtJB2kwR0AAAAeEabNlL37sbDnfJyqaDAdUlNfr60b5+0Zo0R9qvr0KHmunir1SjpaeIXuBLQAQAA0Hj8/b8P0+7YbMbdWd3VxefnS7t2Ge0mbTbH5wYF1VwTHxNjzNYHBnr3OK8CAR0AAADmYrEYtethYVLPnu63KytzbDdZvbRm+3Zp1Srp0iXn5/5fu8mIG26Q/vd/vXYoDUFABwAAQNMUECDFxRkPd2w26exZt3XxftV7xZsAAR0AAADNl8Vi1K536CBdf73T6oKsLHXwwbBqQj8bAAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcAAABMhIAOAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJWGw2m83XgzCTL774QkFBQb4eBgAAAJq50tJS9e3b12k5AR0AAAAwEUpcAAAAABMhoAMAAAAmQkAHAAAATISADgAAAJgIAR0AAAAwEQI6AAAAYCL+vh5Ac7R27Vpt375dWVlZOnjwoC5evKg777xTL730Ur33derUKb366qtKT0/XuXPnFBkZqZEjR2rmzJkKCwvzwuhRV544z2fPnlVqaqo2btyor776SgUFBQoICFD37t01duxYjRs3Tn5+/B3tK578t1zVRx99pN/85jeSpLlz5+qee+7xxHDRQJ4+z1u3btV7772nL774QufPn1f79u3Vo0cPTZ06VbfccouHR4+68uR53rhxo1JSUvT111/r3LlzioiIUK9evfSzn/1M/fr188LoUReefk/1ZQYjoHvBX//6Vx08eFAhISGKiorSkSNHGrSf3NxcTZw4UYWFhRo5cqQSExO1d+9epaSkKD09XUuWLFF4eLiHR4+68sR5Xrt2rX7/+98rIiJCgwcPVnR0tL799lutW7dOc+bMUXp6ul599VVZLBYvHAFq46l/y1WdPHlSf/jDHxQSEqLi4mIPjBJXy5Pn+YUXXtCiRYsUFRWlESNGKDw8XGfOnNH+/fuVmZlJQPchT53nF198UW+++abat2+vUaNGKTw8XLm5uVq/fr0+/fRTPf/88xozZoyHR4+68OR7qs8zmA0et3XrVtvRo0dtFRUVts8//9zWvXt32yOPPFLv/cyYMcPWvXt3W0pKisPy5557zta9e3fb7373O08NGQ3gifO8ZcsWW1pamu3KlSsOy7/55hvbLbfcYuvevbtt7dq1nhw26sFT/5btKioqbNOmTbONHDnS9sc//tHWvXt329KlSz04YjSEp87zBx98YOvevbvt8ccft5WWljqtv3z5sieGiwbyxHn+5ptvbNdee61t2LBhtm+//dZp/927d7eNGDHCk8NGPXjyPdXXGYzPzr1gyJAhSkhIuKpZz9zcXGVkZMhqtWrSpEkO62bNmqWQkBB9/PHHzMD5kCfO89ChQzVixAinj9wiIiI0ceJESdK2bduuapxoOE+c46pSUlL0+eefa968eQoJCfHIPnH1PHGeL1++rFdeeUXR0dF65plnFBgY6LRNQEDA1QwTV8kT5/nEiROqqKhQ79691bFjR6f9t2nTRmfOnLnaoaKBPPWeaoYMRkA3qczMTEnS8OHDnf5HCw0NVf/+/VVSUqI9e/b4YnhoBP7+RgVaq1atfDwSeEJ2drZefvllTZ06VQMHDvT1cOBhn332mc6cOaPbb79dfn5+2rhxoxYuXKh33nlHu3fv9vXw4CHx8fEKCAjQvn37nIL49u3bdfHiRQ0bNsxHo0NN6vOeaoYMRg26Sdlr4xISElyuj4+PV0ZGho4ePaqhQ4c24sjQGMrLy/XRRx9Jkm666SYfjwZXq7y8XI899pi6dOmiX//6174eDrxg3759kqSgoCDdfffd+uqrrxzWDxw4UPPnz1eHDh18MTx4SPv27fXoo4/qj3/8o37yk59o1KhRat++fWUN+o033qhnnnnG18NENfV9TzVDBiOgm1RRUZEkqW3bti7X25dfuHCh0caExvPyyy/rq6++0i233EJAbwZef/11ZWVl6f3331fr1q19PRx4QWFhoSRp0aJFSkpK0uLFi5WcnKy8vDy98MILysjI0K9+9Su9++67Ph4prtb06dMVExOjJ598UkuXLq1cHh8fr7vvvtup9AW+V9/3VDNkMEpcAJNJSUnRW2+9pcTERL3wwgu+Hg6u0p49e/TGG2/Qfq2Zs9lskoyPz//6179qwIABatOmjXr06KHXXntNUVFR2rZtG+UuzcD//u//6pe//KXuvvtupaam6osvvtDKlSsVGxurRx99lN/bJtNU31MJ6CYVGhoqyf1fZ/bl7v66Q9P03nvv6dlnn1W3bt2UkpKi9u3b+3pIuArl5eX6zW9+o4SEBD300EO+Hg68yP67uGfPnoqJiXFYFxwcrOHDh0uS9u7d2+hjg+dkZmbqpZde0ogRI/TEE08oNjZWwcHB6tWrl1577TV17txZb7/9to4fP+7roUINf081QwYjoJtUYmKiJOnYsWMu1+fk5EiSunbt2lhDgpf9/e9/1x/+8Ad1795dKSkpioiI8PWQcJWKi4t17NgxZWdn6/rrr1ePHj0qH6+99pokac6cOerRo4eeffZZH48WV8P+u9jdG3a7du0kSaWlpY02Jnjexo0bJUmDBw92WhccHKzevXuroqJCBw4caOSRobqreU81QwajBt2k7P/4MzIyVFFR4XAVcVFRkXbt2qXg4GD16dPHV0OEBy1cuFAvv/yykpOT9dZbb3EhWTMRGBio8ePHu1x34MABHThwQDfccIO6du1K+UsTN3ToUFksFmVnZzv9zpakw4cPS5LT7DqalsuXL0uS21aK9uW01PStq31PNUMGI6D7WFlZmXJzcxUQEKC4uLjK5XFxcRo+fLgyMjK0ePFiTZkypXLdggULVFxcrAkTJtBLuYlwd54l4wLC+fPnq1evXnrrrbcoa2miXJ3j1q1bu50ZX7BggQ4cOKC7775b99xzT2MOFVfB3b9lq9Wq2267TevXr1dKSoqmT59euS4jI0MZGRlq164dF303Ee7O8w033KD33ntPS5cu1cSJE9W5c+fKdZs2bdKuXbsUFBTEH9w+VJ/3VDNnMIvNfmULPCY1NVWpqamSpNOnTysjI0OxsbEaMGCAJCk8PFyPP/64JCkvL08jR46U1WrV+vXrHfZT/TazSUlJ2rNnjzIzM5WQkKB//OMf3r3NLGrkifP84Ycfavbs2WrVqpUmT57s8uNxq9WqsWPHNsIRoTpP/Vt2ZcGCBXrttdc0d+5cArqPeeo8nzp1ShMnTtTJkyc1dOhQJScnKz8/X6mpqbJYLPrTn/6kH/7wh417cKjkifNcUVGh+++/X1u2bFGbNm10++23q1OnTsrOztbGjRtls9n05JNPatq0aY1/gKj3e6qZMxgz6F6QlZWlDz/80GHZ8ePHKy8asVqtlb8EahIXF6cVK1Zo/vz5Sk9P1+bNmxUREaGpU6dq5syZCgsL88r4UTeeOM95eXmSpCtXruidd95xuc2gQYMI6D7iqX/LMDdPneeoqCitXLlSr7/+utavX68dO3aoTZs2uu222/TAAw+od+/eXhk/6sYT59nPz08LFy7U4sWL9cknn2jdunW6dOmSwsLCdMstt2jKlCmVFwSj8XnyPdXXGYwZdAAAAMBE6OICAAAAmAgBHQAAADARAjoAAABgIgR0AAAAwEQI6AAAAICJENABAAAAEyGgAwAAACZCQAcANLopU6ZoxIgRvh4GAJgSdxIFgGYiMzNTU6dOdbu+VatWOnDgQCOOCADQEAR0AGhm7rjjDt18881Oy/38+NAUAJoCAjoANDM9e/bUmDFjfD0MAEADEdABoIXJy8vTyJEjNXPmTHXt2lVvvPGGjh07po4dO2rcuHH6xS9+IX9/x7eHgwcPasGCBdqxY4eKi4sVGxuru+++WzNmzFCrVq0ctj19+rTeeOMNbdiwQQUFBWrbtq2uvfZa/fznP9eNN97osG1BQYGef/55paen6/LlyxowYIDmzJmjrl27Vm5TWlqqhQsXavXq1Tp16pQCAgLUpUsXDR8+XI8//rj3flAA4CMEdABoZkpKSnTmzBmn5YGBgQoNDa38fv369Tp+/LgmTZqkTp06af369Xrttdd04sQJzZs3r3K7ffv2acqUKfL396/cdsOGDXrppZd08OBBvfzyy5Xb5uXl6b777lNhYaHGjBmj6667TiUlJdqzZ4+2bNniENCLi4s1efJk9enTRw8//LDy8vKUkpKiBx98UKtXr64M/k8//bRWrFihu+66S/369dOVK1d07NgxZWZmeuPHBwA+R0AHgGZmwYIFWrBggdPyW2+9VW+88Ubl9wcPHtTy5cvVq1cvSdLkyZM1c+ZMrVy5UhMmTFDfvn0lSc8++6wuX76sf/zjH7r22msrt33ooYe0evVqjR8/XkOHDpVkhOlvvvlGb775pm666SaH16+oqHD4/uzZs7r//vv1n//5n5XLOnTooBdffFFbtmypfH5qaqpuvvlmPf/881f5kwGApoGADgDNzIQJE/SjH/3IaXmHDh0cvh82bFhlOJcki8Win//850pNTdW6devUt29fFRYWavfu3br99tsrw7l921/84hdau3at1q1bp6FDh+rcuXNKT0/XTTfd5BTOJeeLVP38/Jy6zgwZMkSSlJOTU7mP0NBQff311/rqq6/UvXv3ev40AKDpIaADQDMTHx+vYcOG1bpdUlKS07Ju3bpJko4fPy7JKFmpuryqxMRE+fn5VW6bm5srm82mnj171mmckZGRCgoKcljWvn17SdK5c+cqlz355JP6zW9+ozvvvFOxsbEaPHiwbrvtNo0YMYLONACaJQI6AMAnql9cWpXNZqv871GjRmn9+vXatGmTtm/fri1btmj58uUaMGCA3n77bQUGBjbGcAGg0TD1AAAtVHZ2ttOyr7/+WpIUGxsrSYqJiXFYXtWRI0dUUVFRuW1cXJwsFouysrI8Ptb27dtrzJgxmjt3rtLS0vTzn/9cO3bsUFpamsdfCwB8jYAOAC3Uli1btH///srvbTab3nzzTUnGrLUkdezYUf369dOGDRv01VdfOWy7cOFCSdLtt98uyQjRN998szZv3qwtW7Y4vV7VWfG6unLlir777juHZRaLpbKM5vz58/XeJwCYHSUuANDMHDhwQB999JHLdfbgLUnXXnutpk2bpkmTJikiIkJpaWnasmWLxowZo379+lVu99vf/lZTpkzRpEmT9NOf/lQRERHasGGDMjIydMcdd1R2cJGk3/3udzpw4ID+8z//U3fddZd69eql0tJS7dmzR1arVY899li9juXixYsaPny4RowYoZ49e6pDhw7Ky8vTkiVLFBYWpttuu62ePx0AMD8COgA0M6tXr9bq1atdrvv0008ra79HjBhReaOio0ePqmPHjnrwwQf14IMPOjzn+uuv1z/+8Q/Nnz9fS5YsqbxR0aOPPqoZM2Y4bBsbG6sVK1bo9ddf1+bNm/XRRx+pXbt2uvbaazVhwoR6H0vr1q01bdo0bd26VVu3btXFixcVGRmpESNG6IEHHlDnzp3rvU8AMDuLrSGfOQIAmqyqdxKdNWuWr4cDAKiGGnQAAADARAjoAAAAgIkQ0AEAAAAToQYdAAAAMBFm0AEAAAATIaADAAAAJkJABwAAAEyEgA4AAACYCAEdAAAAMBECOgAAAGAi/x9XYCnQoykSSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')  # Set the plot style\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the lines\n",
    "plt.plot(range(1, num_epochs + 1), learning_rate_curve.train_losses, label=\"Train loss\",color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), learning_rate_curve.val_losses, label=\"Val loss\",color='red')\n",
    "plt.plot(range(1, num_epochs + 1), learning_rate_curve.f1_scores, label=\"Val F1\",color='green')\n",
    "\n",
    "# Set the x and y labels and font sizes\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "\n",
    "# Add a legend and customize it\n",
    "plt.legend(loc='best', fontsize=50)\n",
    "legend = plt.legend(fontsize=20)\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "# Show the plot\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zls-paCKBwl-"
   },
   "source": [
    "# Dev set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnU6_EEIBZK2",
    "outputId": "2e3df395-21a6-4e87-c57d-418f0a5c78cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /content/drive/MyDrive/NLPClassification_40/models/pcl_roberta_finetuned/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaPCL\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file /content/drive/MyDrive/NLPClassification_40/models/pcl_roberta_finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaPCL.\n",
      "\n",
      "All the weights of RobertaPCL were initialized from the model checkpoint at /content/drive/MyDrive/NLPClassification_40/models/pcl_roberta_finetuned/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaPCL for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if model's already trained\n",
    "\n",
    "model_name = '/content/drive/MyDrive/NLPClassification_40/models/pcl_roberta_finetuned/'\n",
    "model = RobertaPCL.from_pretrained(model_name).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if7xZDj3Bso9",
    "outputId": "95f05152-55be-4a7f-ad78-7840ff991c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1900, 1: 193})\n",
      "0.5612244897959183\n"
     ]
    }
   ],
   "source": [
    "test_dataset = PCLDataset(tokenizer, test_set, augment=True)\n",
    "\n",
    "test_dataset_short = PCLDataset(tokenizer, test_set[test_set['length'] > 0].reset_index(drop=True), augment=True)\n",
    "test_loader = DataLoader(test_dataset_short , batch_size=32, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "tot_preds = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in (test_loader):\n",
    "        batch = batch.to(torch.device('cuda'))\n",
    "        label = batch.pop('label')\n",
    "        extra = batch.pop('extra')\n",
    "        logits = model(extra, **batch)\n",
    "        \n",
    "        preds = logits.argmax(axis=1)\n",
    "        tot_preds.extend(list(preds.cpu().numpy()))\n",
    "        labels.extend(list(label.cpu().numpy()))\n",
    "\n",
    "print(Counter(tot_preds))\n",
    "\n",
    "print(f1_score(labels, tot_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "-iXdjvpFJlNU",
    "outputId": "315aed8e-1051-4548-c31e-1998ee16edbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.3319672131147541\n",
      "recall： 0.8140703517587939\n",
      "f1_score: 0.47161572052401735\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "print('precision: ' + str(precision_score(labels, tot_preds)))\n",
    "print( 'recall： ' + str(recall_score(labels, tot_preds)))\n",
    "print('f1_score: ' + str(f1_score(labels, tot_preds)))\n",
    "\n",
    "# Display the confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(labels, tot_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2H34MU_z6Vl",
    "outputId": "17dd5013-137a-43ef-a027-4635b86d3e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development set length = 2093\n",
      "Predictions  length = 2093\n"
     ]
    }
   ],
   "source": [
    "print(f'Development set length = {len(test_set)}')\n",
    "print(f'Predictions  length = {len(tot_preds)}')\n",
    "\n",
    "labels2file([[k] for k in tot_preds], './res/task1.txt')\n",
    "labels2file(test_set.label.apply(lambda x:[x]).tolist(), './ref/task1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdSmlo5u9Bo1",
    "outputId": "fe3a169c-23ae-4b93-dc2c-76dec29b83a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "module_url = f\"https://raw.githubusercontent.com/Perez-AlmendrosC/dontpatronizeme/master/semeval-2022/evaluation.py\"\n",
    "module_name = module_url.split('/')[-1]\n",
    "print(f'Fetching {module_url}')\n",
    "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
    "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
    "  a = f.read()\n",
    "  outf.write(a.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81wSxvl_BrCV"
   },
   "source": [
    "## Scores for different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVDOTUU7waci",
    "outputId": "715e390a-873c-45f6-9d3d-faea37cd9bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4794520547945206\n"
     ]
    }
   ],
   "source": [
    "# f1 for different lengths\n",
    "\n",
    "short_len = 0.05\n",
    "long_len = 0.25 \n",
    "\n",
    "test_dataset_short = PCLDataset(tokenizer, test_set[test_set['length'] > long_len].reset_index(drop=True), augment=True)\n",
    "test_loader = DataLoader(test_dataset_short , batch_size=32, collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(torch.device('cuda'))\n",
    "        label = batch.pop('label')\n",
    "        extra = batch.pop('extra')\n",
    "        \n",
    "        logits = model(extra, **batch)\n",
    "        \n",
    "        preds.extend(list(logits.argmax(dim=-1).cpu().numpy()))\n",
    "        labels.extend(list(label.cpu().numpy()))\n",
    "\n",
    "\n",
    "print(f1_score(labels, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjgXqN3sw1j3"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSZD99YGFI4R"
   },
   "outputs": [],
   "source": [
    "submission_set['label'] = -1\n",
    "\n",
    "submission_dataset = PCLDataset(tokenizer, submission_set, augment=True)\n",
    "submission_loader = DataLoader(submission_dataset, batch_size=32, collate_fn=submission_dataset.collate_fn)\n",
    "\n",
    "tot_preds = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in (submission_loader):\n",
    "        batch = batch.to(torch.device('cuda'))\n",
    "        labels = batch.pop('label')\n",
    "        extra = batch.pop('extra')\n",
    "        logits = model(extra, **batch)\n",
    "        \n",
    "        preds = logits.argmax(axis=1)\n",
    "        tot_preds.extend(list(preds.cpu().numpy()))\n",
    "\n",
    "\n",
    "labels2file([[k] for k in tot_preds], './task1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kJT-87PIhPA"
   },
   "outputs": [],
   "source": [
    "!zip submission.zip task1.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f21c31d526546c3b5e6c679a8c40060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22346120d609431eb7933db08c41092c",
      "placeholder": "​",
      "style": "IPY_MODEL_a9a8248b51da49dca255ca56f751453e",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "15f0070dfd3e4f98a9ee02f6e6409bc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22346120d609431eb7933db08c41092c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28822e57532a42c0923bea868681f340": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3918797b878946fb8ed30364f10c951b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39d2f76ced4d434ba462d61554d85bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3918797b878946fb8ed30364f10c951b",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_28822e57532a42c0923bea868681f340",
      "value": 481
     }
    },
    "3a6f951883ce43bebbb76640b0eaa8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f21c31d526546c3b5e6c679a8c40060",
       "IPY_MODEL_39d2f76ced4d434ba462d61554d85bff",
       "IPY_MODEL_48d9b89e8d814695bd607f73cb5a37ad"
      ],
      "layout": "IPY_MODEL_9e99cea34c0c4cb9a34f3ef66b922c6b"
     }
    },
    "48d9b89e8d814695bd607f73cb5a37ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15f0070dfd3e4f98a9ee02f6e6409bc6",
      "placeholder": "​",
      "style": "IPY_MODEL_79c817f57f814cb9825bb2d2afff1dc2",
      "value": " 481/481 [00:00&lt;00:00, 8.12kB/s]"
     }
    },
    "79c817f57f814cb9825bb2d2afff1dc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e99cea34c0c4cb9a34f3ef66b922c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9a8248b51da49dca255ca56f751453e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
